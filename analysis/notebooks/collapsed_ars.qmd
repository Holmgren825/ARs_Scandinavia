---
title: Collapsed ARs
author: Erik Holmgren
format:
  html:
    code-fold: show
    toc: true
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.16.4
  kernelspec:
    display_name: ar_ident
    language: python
    name: ar_ident
---

```{python}
%load_ext autoreload
```

```{python}
%autoreload 2
```

```{python}
import re
from glob import glob
from itertools import combinations
from pathlib import Path

import cf_xarray
import dask.array as da
import numpy as np
import pandas as pd
import scipy
import ultraplot as plt
import xarray as xr
from ar_identify.metrics import spatial_jaccard_score
from ar_scandinavia.pca_utils import combine_artmip_pca_results
from ar_scandinavia.utils import (
    compute_ar_pr_values_collapsed,
    compute_spatial_correltion,
    subsel_ds,
)
from cartopy import crs as ccrs
from dask_ml.cluster import KMeans
from distributed.client import Client
from tqdm.autonotebook import tqdm
from xr_utils.coordinates import conv_lon_to_180
```

```{python}
plt.rc["font.size"] = 7
```

```{python}
LAT_SLICE = (50, 74)
LON_SLICE = (-10, 45)
```

```{python}
client = Client()
```

```{python}
precip_path = "/data/era5/total_precipitation/total_precipitation-*6h.zarr/"
precip_ds = xr.open_mfdataset(precip_path, engine="zarr")
```

```{python}
precip_ds = precip_ds.cf.sel(time=slice("1980", "2019"))
```

```{python}
precip_ds = subsel_ds(precip_ds, LAT_SLICE, LON_SLICE)
```

```{python}
# NOTE: also convert to mm.
ann_avg_precip_ds = (
    precip_ds.tp.groupby("valid_time.year").sum().mean("year").load() * 1e3
)
ann_avg_precip_ds.attrs["units"] = "mm"
```

Get annual time steps array.

```{python}
n_days = precip_ds.valid_time.groupby("valid_time.year").count()
```

# Read in data

First we get the timesteps for the differnt ARDTs

```{python}
import os
```

```{python}
N_CLUSTERS = 6
```

```{python}
BASE_PATH = "/data/projects/atmo_rivers_scandinavia/"
ARDT_NAMES = ["Mundhenk_v3", "Reid500", "GuanWaliser_v2", "TempestLR"]
```

```{python}
def remap_labels(labels: da.array, label_dict: dict) -> da.array:
    return labels.map_blocks(
        lambda x: np.vectorize(label_dict.get)(x),
        dtype=int,
    )
```

```{python}
def get_cluster_timesteps(
    cluster_ds: xr.Dataset,
    precip_ds: xr.Dataset,
    ardt_name: str,
    remap_labels: bool = True,
) -> xr.Dataset:
    """Get a dataset with all timesteps for the clusters in cluster_ds."""
    curr_times = np.asarray(
        list(
            map(
                lambda x: x[-23:].split("-"),
                cluster_ds.sample_id.values,
            )
        )
    )
    labels = cluster_ds.labels.load()
    # NOTE: Here we just get the timesteps.
    ar_pr_timesteps = []
    padded_labels = []
    for i, time in enumerate(curr_times):
        pr_ts = precip_ds.tp.cf.sel(
            time=slice(time[0], time[1]),
        ).cf["time"]
        curr_label = labels.isel(sample_id=[i]).values[0]
        # NOTE: Here we re-map the labels to common geogrpahical feature.
        if remap_labels:
            curr_label = label_dict[ardt_name][curr_label]
        padded_labels.extend([curr_label] * pr_ts.shape[0])
        ar_pr_timesteps.append(pr_ts)

    ar_pr_timesteps_ds = xr.concat(ar_pr_timesteps, dim="valid_time")
    padded_labels = np.asarray(padded_labels)
    assert padded_labels.shape[0] == ar_pr_timesteps_ds.shape[0]

    ar_pr_timesteps_ds = xr.DataArray(
        padded_labels, coords={"valid_time": ar_pr_timesteps_ds.valid_time}
    )
    return ar_pr_timesteps_ds
```

First we have to read in data with the original labels

```{python}
ar_ens_ds = []
for ardt_name in tqdm(ARDT_NAMES):
    ar_path = os.path.join(
        BASE_PATH,
        f"ERA5.{ardt_name}",
        f"ERA5.{ardt_name}.scand_ars.collapsed.1980-2019.zarr",
    )
    label_path = os.path.join(
        BASE_PATH,
        f"ERA5.{ardt_name}",
        f"ERA5.{ardt_name}.collapsed.cluster_labels.zarr",
    )
    ar_ds = xr.open_zarr(ar_path)
    label_ds = xr.open_zarr(label_path)
    ar_ds = subsel_ds(ar_ds, LAT_SLICE, LON_SLICE)

    curr_times = np.asarray(
        list(
            map(
                lambda x: x[-23:].split("-"),
                ar_ds.sample_id.values,
            )
        )
    )
    start_times = pd.to_datetime(curr_times[:, 0], format="%Y%m%dT%H")
    labels = label_ds.labels.data

    ar_ds = ar_ds.assign_coords({"start_time": ("sample_id", start_times)})
    ar_ds = ar_ds.assign_coords({"label": ("sample_id", labels)})

    ar_ens_ds.append(ar_ds)

ar_ens_ds = xr.concat(ar_ens_ds, dim="ardt")
```

Here we compute the average of each ARDTs/cluster.
This is used to perform the field correlation between the clusters of the different ARDTs.

```{python}
cluster_eval = (
    ar_ens_ds.groupby(("label", "ardt", "start_time.year")).sum().mean("year").load()
)
```

```{python}
fig, ax = plt.subplots(
    nrows=N_CLUSTERS,
    ncols=4,
    proj="lcc",
    proj_kw={"central_longitude": 15},
    share=False,
)
for i in range(N_CLUSTERS):
    for j in range(4):
        (cluster_eval.isel(label=i, ardt=j).ar_tracked_id / 1464 * 100).plot(
            ax=ax[i, j]
        )

ax.format(
    coast=True,
    lonlim=LON_SLICE,
    latlim=LAT_SLICE,
    title="",
    rowlabels=list(cluster_eval.label.values.astype(int)),
    collabels=list(cluster_eval.ardt.values),
)
```

Compute the correlation between clusters of different ARDTs.

```{python}
label_dict = {}
for ardt in range(1, 4):
    sub_res = {}
    for true_cluster in range(N_CLUSTERS):
        best_score = 0
        for test_cluster in range(N_CLUSTERS):
            score = compute_spatial_correltion(
                cluster_eval.isel(ardt=0, label=test_cluster).ar_tracked_id,
                cluster_eval.isel(ardt=ardt, label=true_cluster).ar_tracked_id,
            )
            if score > best_score:
                best_score = score
                sub_res[true_cluster] = test_cluster
        label_dict[ARDT_NAMES[ardt]] = sub_res

label_dict["Mundhenk_v3"] = {val: val for val in range(5)}
```

```{python}
label_dict
```

Here we load in the clustered AR data again, this time we also re-map the cluster labels.

```{python}
ar_ens_ds = []
for ardt_name in tqdm(ARDT_NAMES):
    ar_path = os.path.join(
        BASE_PATH,
        f"ERA5.{ardt_name}",
        f"ERA5.{ardt_name}.scand_ars.collapsed.1980-2019.zarr",
    )
    label_path = os.path.join(
        BASE_PATH,
        f"ERA5.{ardt_name}",
        f"ERA5.{ardt_name}.collapsed.cluster_labels.zarr",
    )
    ar_ds = xr.open_zarr(ar_path)
    label_ds = xr.open_zarr(label_path)
    ar_ds = subsel_ds(ar_ds, LAT_SLICE, LON_SLICE)

    curr_times = np.asarray(
        list(
            map(
                lambda x: x[-23:].split("-"),
                ar_ds.sample_id.values,
            )
        )
    )
    start_times = pd.to_datetime(curr_times[:, 0], format="%Y%m%dT%H")
    labels = remap_labels(label_ds.labels.data, label_dict[ardt_name])

    ar_ds = ar_ds.assign_coords({"ardt": [ardt_name]})
    ar_ds = ar_ds.assign_coords({"start_time": ("sample_id", start_times)})
    ar_ds = ar_ds.assign_coords({"label": ("sample_id", labels)})

    ar_ens_ds.append(ar_ds)

ar_ens_ds = xr.concat(ar_ens_ds, dim="ardt")
```

Compute the new cluster evaluation and plot the clusters for all ARDTs to see how well they match.

```{python}
cluster_eval = (
    ar_ens_ds.groupby(("label", "ardt", "start_time.year")).sum().mean("year").load()
)
```

```{python}
n_clusters = cluster_eval.label.shape[0]
fig, ax = plt.subplots(
    nrows=n_clusters,
    ncols=4,
    proj="lcc",
    proj_kw={"central_longitude": 15},
    share=False,
)
for i in range(n_clusters):
    for j in range(4):
        (cluster_eval.isel(label=i, ardt=j).ar_tracked_id / 1464 * 100).plot(
            ax=ax[i, j]
        )

ax.format(
    coast=True,
    lonlim=LON_SLICE,
    latlim=LAT_SLICE,
    title="",
    rowlabels=list(cluster_eval.label.values.astype(int)),
    collabels=list(cluster_eval.ardt.values),
)
```

## Get time steps

```{python}
ar_ts_ens = []
for ardt_name in tqdm(ARDT_NAMES):
    cluster_path = os.path.join(
        BASE_PATH,
        f"ERA5.{ardt_name}",
        f"ERA5.{ardt_name}.collapsed.cluster_labels.zarr",
    )
    cluster_ds = xr.open_zarr(cluster_path)

    ar_ts_ds = get_cluster_timesteps(cluster_ds, precip_ds, ardt_name)
    ar_ts_ds = ar_ts_ds.assign_coords({"ardt": ardt_name})
    ar_ts_ens.append(ar_ts_ds)
ar_ts_ens = xr.concat(ar_ts_ens, dim="valid_time")
```

Why does 3 algos have more than half duplicate values, where on has only 500???
This is potentially a bug in xarray: https://github.com/pydata/xarray/issues/8499

```{python}
n_ar_ts_per_ardt = list(map(len, ar_ts_ens.groupby("ardt").groups.values()))
print(n_ar_ts_per_ardt)
```

```{python}
n_ar_ts_per_ardt = list(
    map(len, ar_ts_ens.drop_duplicates("valid_time").groupby("ardt").groups.values())
)
print(n_ar_ts_per_ardt)
```

How many duplicate time steps are there truly?
If we do this outside the loop we get completely different results.

```{python}
for name in ARDT_NAMES:
    ts1 = ar_ts_ens.where(ar_ts_ens.ardt == name, drop=True).shape[0]
    ts2 = (
        ar_ts_ens.where(ar_ts_ens.ardt == name, drop=True)
        .drop_duplicates("valid_time")
        .shape[0]
    )
    print(ts1, ts2)
```

# AR precipitation

Here we just get all precipitation time steps from when there are ARs.

We can use these AR time steps to select the precipitation time steps that occur at the same time.
At this points, we don't really need to care about the ensemble members, we can just pile all of it into a single large 1d array? Yes, we group them by ardt later.
- For the histograms yes, but not for the maps? 
- But we can remove the time coordinate for now, but it would be nice to keep the groupby functionality.

```{python}
ar_precip_ens = precip_ds.cf.sel(time=ar_ts_ens.valid_time) * 1e3

n_ar_precip_ens = (
    precip_ds.where(~precip_ds.cf["time"].isin(ar_ts_ens.valid_time), drop=True) * 1e3
)

n_ar_precip_ens.tp.attrs["units"] = "mm"
ar_precip_ens.tp.attrs["units"] = "mm"
```

Remove the average non-AR precip from the AR precip.

```{python}
n_ar_precip_avg_ts = n_ar_precip_ens.mean("valid_time")
```

```{python}
ar_precip_ens = ar_precip_ens - n_ar_precip_avg_ts
ar_precip_ens = ar_precip_ens.where(ar_precip_ens > 0, 0)
```

Remove duplicate time steps.

```{python}
dedup = []
for name in ARDT_NAMES:
    dedup.append(
        ar_precip_ens.where(ar_precip_ens.ardt == name, drop=True).drop_duplicates(
            "valid_time"
        )
    )
ar_precip_ens = xr.concat(dedup, dim="valid_time")
```

Assurance check

```{python}
# These are the timesteps that show in any of the AR algos., but with not repeats.
n_ar_ts_no_dup = ar_precip_ens.cf.drop_duplicates("time").cf["time"].shape[0]
# Timesteps during AR and not during AR.
total_timesteps = n_ar_ts_no_dup + n_ar_precip_ens.cf["time"].shape[0]
# Should be equal the total number time steps between 1980 and 2019.
assert total_timesteps == precip_ds.cf["time"].shape[0]
```

## Tests
What does the total precipitation for the different ARDTs look like?
What we want to check here is if the respective precipitation charts look reasonable with regards to their frequencies.

```{python}
tp_test = ar_precip_ens.cf.groupby(["time.year", "ardt"]).sum().mean("year").load()
```

```{python}
ncols = 4
fig, ax = pplt.subplots(
    ncols=ncols,
    proj=ncols * ["lcc"],
    proj_kw={"central_longitude": 15},
    abc=True,
)
cmap = pplt.Colormap("oslo_r", right=0.8)
for i in range(4):
    (
        # tp_test.isel(ardt=i).tp
        (tp_test.isel(ardt=i).tp / ann_avg_precip_ds * 100).plot(
            ax=ax[i],
            cmap=cmap,
            vmin=0,
            rasterized=True,
            cbar_kwargs={"label": "Total precip. fraction [%]"},
        )
    )

ax.format(
    coast=True,
    reso="med",
    # title=ARDT_NAMES,
    lonlim=LON_SLICE,
    latlim=LAT_SLICE,
)
```

This is actually quite interesting.
Why is there such a large spatial difference between the average precipitation field for the different algorithms?
Also, important to remember that this only indicates that there are ARs somewhere in the region when this precipitation occurs.
It doesn't mean that the ARs and precipitation are spatially co-located.
This is why we compute the spatial correlation later.
Wonder if it does make sense to compute some type of space-time correlation?

```{python}
fig.savefig("../../figures/tp_per_ardt_check.pdf")
```

# Ensemble AR frequency and precipitation

## Annual average

```{python}
n_ar_ann_avg_pr = (
    n_ar_precip_ens.cf.groupby("time.year").sum().mean("year").tp.compute()
)

ar_ann_avg_pr = (
    ar_precip_ens.cf.groupby(["time.year", "ardt"])
    .sum()
    .mean("year")
    # .median("ardt")
    .tp.compute()
)
```

How many AR time steps do we get during the average year?

```{python}
ar_ann_avg_frq = (
    (ar_ens_ds.groupby(["ardt", "start_time.year"]).sum() / n_days.values).mean("year")
    # .median("ardt")
    .load()
)
```

```{python}
assert np.all(ar_ann_avg_pr.ardt.values == ar_ann_avg_frq.ardt.values)
```

The field correlation should be computed on a per member basis like below.

```{python}
corr = compute_spatial_correltion(
    ar_ann_avg_frq.ar_tracked_id,
    (ar_ann_avg_pr / ann_avg_precip_ds * 100),
    dim=["lat", "lon"],
)
```

Get ensemble correlation spread

```{python}
corr = corr.quantile([0, 0.5, 1])
```

```{python}
corr
```

```{python}
# Save the correlation
corr.to_pandas().to_csv(
    "/data/projects/atmo_rivers_scandinavia/ensemble_correlations.csv"
)
```

```{python}
def annual_average_plot(ar_ds, pr_ds, suptitle, ann_avg_precip_ds):
    fig, axs = plt.subplots(
        figwidth="8.3cm",
        nrows=3,
        # proj=2 * ["nsper"],
        # proj_kw={"lon_0": 14, "lat_0": 65},
        proj=3 * ["lcc"],
        proj_kw={"central_longitude": 15},
        abc=True,
        share=False,
    )
    (ar_ds.ar_tracked_id * 100).plot(
        ax=axs[0],
        vmin=0,
        cbar_kwargs={"label": "AR frequency [%]"},
        rasterized=True,
        cmap="bamako",
    )
    # vmax = max(n_ar_ann_avg.max().values, ar_ann_avg.max().values)
    cmap = plt.Colormap("oslo_r", right=0.8)
    cm = pr_ds.plot(
        ax=axs[1], vmin=0, cmap=cmap, add_colorbar=False, rasterized=True, robust=False
    )
    # cm = ar_ann_avg.plot(ax=axs[1], vmin=0, vmax=vmax, cmap="oslor_r", add_colorbar=False)
    axs[1].colorbar(cm, label="Total annual average\nprecipitation [mm]")

    (pr_ds / ann_avg_precip_ds * 100).plot(
        ax=axs[2],
        vmin=0,
        cmap=cmap,
        cbar_kwargs={"label": "Total precipitation\nfraction [%]"},
        rasterized=True,
    )

    axs.format(
        coast=True,
        lonlabels=True,
        latlabels=True,
        reso="med",
        lonlim=LON_SLICE,
        latlim=LAT_SLICE,
        title="",
        rowlabels=[
            "AR frequency",
            "Total AR precipitation",
            "AR precipitation fraction",
        ],
        suptitle=suptitle,
        abcloc="ul",
        abcbbox=True,
        abc_kw={"bbox": {"boxstyle": "square", "facecolor": "white"}},
    )
    return fig
```

```{python}
fig = annual_average_plot(
    ar_ann_avg_frq.median("ardt"),
    ar_ann_avg_pr.median("ardt"),
    "Annual average precipitation\nduring ARs [Ensemble median]",
    ann_avg_precip_ds,
)
```

```{python}
fig.savefig("../../figures/ann_avg_ar_pr_ens_med.svg")
```

#### ECDF test figure 

```{python}
n_days_groups = n_days.values.reshape((1, 1, -1, 1, 1))
```

```{python}
# test = (ar_ens_ds.groupby(["start_time.year", "ardt", "label"]).sum() / n_days_groups).ar_tracked_id.compute()
test = (ar_ens_ds.groupby(["start_time.year", "ardt"]).sum() / n_days).compute()
```

```{python}
# test_med = test.sum("label").median("ardt")
test_med = test.median("ardt").ar_tracked_id
```

```{python}
cmap = plt.Colormap("berlin", samples=40)
```

```{python}
fig, ax = plt.subplots(figheight="10cm")
for i in range(0, 40):
    data = test_med.isel(year=i).values.flatten() * 100
    ax.ecdf(data, complementary=True, color=cmap.colors[i], lw=0.7, alpha=0.9)

ax.ecdf(
    test_med.median("year").values.flatten() * 100,
    complementary=True,
    ls="--",
    c="k",
    label="Median",
)

fig.colorbar(cmap, values=range(1980, 2019), label="Year", alpha=0.9)
fig.legend(loc="b")
ax.format(
    suptitle="ECDF of annual AR frequency",
    xlabel="Annual average AR frequency [%]",
    ylabel="Probability of occurrence",
)
```

```{python}
i
```

```{python}
import scipy.stats as scstats
```

```{python}
all_res = np.zeros(40)
for i in range(40):
    data = test_med.isel(year=i).values.flatten() * 100
    res = scstats.ecdf(data).sf
    all_res[i] = np.trapezoid(res.probabilities, dx=np.diff(res.quantiles))
```

```{python}
fig, ax = plt.subplots()
ax.scatter(range(1980, 2020), all_res)
```

### Tests
Check what the AR frequencies for the different ARDTs look like.
The ensemble average above should reflect the ARDTs below.

```{python}
test = ar_ens_ds.groupby(["ardt", "start_time.year"]).sum().mean("year").load()
```

```{python}
n_total_timesteps = 58440
fig, ax = plt.subplots(ncols=4)
for i in range(4):
    (test.isel(ardt=i).ar_tracked_id / 1460 * 100).plot(ax=ax[i], rasterized=True)
ax.format(coast=True)
```

```{python}
fig.savefig("../../figures/ann_frq_per_ardt_check.pdf")
```

These frequencies seem more reasonable when compared to literature.

Read in a single ARDT collapsed clusters.
Here we don't do any fancy things, e.g. grouping etc.
Want to compare the annual average to a single year.
Should roughly match.

```{python}
test = xr.open_zarr(
    "/data/projects/atmo_rivers_scandinavia/ERA5.Reid500/ERA5.Reid500.scand_ars.collapsed.1980-2019.zarr/"
)
```

```{python}
test = subsel_ds(test, LAT_SLICE, LON_SLICE)
```

```{python}
curr_times = np.asarray(
    list(
        map(
            lambda x: x[-23:].split("-"),
            test.sample_id.values,
        )
    )
)
start_times = pd.to_datetime(curr_times[:, 0], format="%Y%m%dT%H")
test = test.assign_coords({"time": ("sample_id", start_times)})
test = test.set_xindex("time")
test_avg = test.reset_index("time").groupby("time.year").sum().mean("year")
```

The subplots should have roughly equal AR frequencies.

```{python}
fig, ax = pplt.subplots(ncols=2)
(test_avg.ar_tracked_id / 1464 * 100).plot(ax=ax[0])
(test.sel(time="1980").sum("sample_id").ar_tracked_id / 1464 * 100).plot(ax=ax[1])
ax.format(title=["Groupby average", "Single year"])
```

Testing out some trends.

```{python}
# This is the wrong order of operations. We need to groupby year first!
# We know this since the magnitude of the resulting max values doesn't
# correspond to what we see in the maps above.

# trend_test = (
#     test.reset_index("time").cf.max(["latitude", "longitude"]).ar_tracked_id.load()
# )
```

```{python}
weights = np.cos(np.deg2rad(test.lat))
trend_test = (
    test.reset_index("time")
    .groupby("time.year")
    .sum()
    .cf.max(["latitude", "longitude"])
    # .cf.weighted(weights).mean(["latitude", "longitude"])
    .ar_tracked_id.load()
)
```

```{python}
(trend_test.rolling(year=1, center=True).mean() / 1464 * 100).plot()
```

```{python}
hov_test = (
    test.reset_index("time")
    .groupby(["time.year", "time.month"])
    .sum()
    .cf.max(["latitude", "longitude"])
    .ar_tracked_id.load()
)
```

```{python}
(hov_test / (1464 / 12) * 100).plot()
```

### Ensemble spread
Groupby `ardt` as before, but now reduce the resulting dimension using the min/max.
#### Max

```{python}
fig = annual_average_plot(
    ar_ann_avg_frq.max("ardt"),
    ar_ann_avg_pr.max("ardt"),
    "Annual average precipitation\nduring ARs [Ensemble max]",
    ann_avg_precip_ds,
)
```

```{python}
fig.savefig("../../figures/ann_avg_ar_pr_ens_max.svg")
```

#### Min

```{python}
fig = annual_average_plot(
    ar_ann_avg_frq.min("ardt"),
    ar_ann_avg_pr.min("ardt"),
    "Annual average precipitation\nduring ARs [Ensemble min]",
    ann_avg_precip_ds,
)
```

```{python}
fig.savefig("../../figures/ann_avg_ar_pr_ens_min.svg")
```

## Trends

```{python}
import statsmodels.api as sm
```

```{python}
weights = np.cos(np.deg2rad(ar_ens_ds.lat))

ar_trend = ar_ens_ds.ar_tracked_id.groupby(["start_time.year", "ardt"]).sum().weighted(
    weights
).mean(["lat", "lon"]) / n_days.values.reshape((-1, 1))
ar_trend = ar_trend.compute()
```

```{python}
weights = np.cos(np.deg2rad(ar_ens_ds.lat))
ar_trend_max = ar_ens_ds.groupby(["start_time.year", "ardt"]).sum().weighted(
    weights
).quantile(0.99, dim=["lat", "lon"]).ar_tracked_id / n_days.values.reshape((-1, 1))
ar_trend_max = ar_trend_max.compute()
```

```{python}
import pymannkendall as mk
```

```{python}
mk_res_avg = []
for ardt_data in ar_trend.values.T:
    results = mk.original_test(ardt_data)
    mk_res_avg.append([results.p, results.slope])
mk_res_ext = []
for ardt_data in ar_trend_max.values.T:
    results = mk.original_test(ardt_data)
    mk_res_ext.append([results.p, results.slope])
```

```{python}
mk_res_avg
```

```{python}
mk_res_ext
```

```{python}
weights = np.cos(np.deg2rad(precip_ds.latitude))
ann_ar_precip = (
    ar_precip_ens.cf.groupby(["time.year", "ardt"])
    .sum()
    .tp.compute()
    .weighted(weights)
    .mean(["latitude", "longitude"])
    # .median("ardt")
    / 1e3
)
ann_ar_precip_max = (
    ar_precip_ens.cf.groupby(["time.year", "ardt"])
    .sum()
    .tp.compute()
    .weighted(weights)
    .quantile(0.99, dim=["latitude", "longitude"])
    # .median("ardt")
    / 1e3
)
```

```{python}
weights = np.cos(np.deg2rad(precip_ds.latitude))
ann_precip = (
    precip_ds.groupby("valid_time.year")
    .sum()
    .tp.compute()
    .weighted(weights)
    .mean(["latitude", "longitude"])
)
ann_precip_max = (
    precip_ds.groupby("valid_time.year")
    .sum()
    .tp.compute()
    .weighted(weights)
    .quantile(0.99, dim=["latitude", "longitude"])
)
```

```{python}
import scipy.stats as sc_stats
```

#### Initial trend plot

```{python}
grid_spec = [[1, 2], [3, 3]]
fig, ax = plt.subplots(
    nrows=3, figwidth="12cm", figheight="12cm", sharey=True, spany=False, abc=True
)

time = ar_trend.year

# Spatial averages
for i in range(4):
    label = f"{ar_trend.isel(ardt=i).ardt.values}\n$p_{{avg}}$: {mk_res_avg[i]:.2f}\n$p_{{ext}}$: {mk_res_ext[i]:.2f}"
    (ar_trend.isel(ardt=i) * 100).plot(ax=ax[0], label=label)
    (ar_trend_max.isel(ardt=i) * 100).plot(ax=ax[1])


# ann_precip.plot(ax=ax[1, 0], c="C5")
ax[2].bar(
    ann_precip.year,
    ann_precip_max.median("ardt").values,
    label="Spatial max",
    cycle="colorblind10",
    c="C0",
)
ax[2].bar(
    ann_precip.year,
    ann_precip.median("ardt").values,
    label="Spatial avg.",
    cycle="colorblind10",
    c="C11",
)
# ann_precip_max.plot(ax=ax[1, 0], c="C5")
ax[2].legend()

handles, labels = ax[0, 0].get_legend_handles_labels()
fig.legend(handles, labels, loc="b", ncols=4)
ax[:2].format(ylabel="Annual AR frequency [%]")
ax[2].format(ylabel="Total annual precipitation [m]")
ax.format(
    xlabel="Year",
    # ylabel=["Ann. AR frequency [%]", "Test"],
    suptitle="Annual AR frequency and total AR precipitation",
    # collabels=["Spatial average", "Spatial 99th percentile"],
    title="",
    abcloc="ul",
    abcbbox=True,
    abc_kw={"bbox": {"boxstyle": "square", "facecolor": "white"}},
)
```

```{python}
fig.savefig("../../figures/ar_ann_avg_trends.svg")
```

#### Alternative trends plot.

Correlation between AR freq. and total annual precip.

```{python}
res = []
for i in range(4):
    ardt = ar_trend.isel(ardt=i).ardt.values
    corr = sc_stats.pearsonr(ar_trend.isel(ardt=i).values, ann_precip_max.values)
    res.append([ardt, corr.statistic, corr.pvalue])

corr_df = pd.DataFrame(res, columns=["ardt", "r2", "p-val"])
corr_df.to_csv(
    "/data/projects/atmo_rivers_scandinavia/ann_ar_avg_pr_max_correlations.csv"
)
corr_df
```

```{python}
res = []
for i in range(4):
    ardt = ar_trend_max.isel(ardt=i).ardt.values
    corr = sc_stats.pearsonr(ar_trend_max.isel(ardt=i).values, ann_precip_max.values)
    res.append([ardt, corr.statistic, corr.pvalue])

corr_df = pd.DataFrame(res, columns=["ardt", "r2", "p-val"])
corr_df.to_csv(
    "/data/projects/atmo_rivers_scandinavia/ann_ar_max_pr_max_correlations.csv"
)
corr_df
```

Correlation between AR freq and AR precip.

```{python}
res = []
for i in range(4):
    ardt = ar_trend_max.isel(ardt=i).ardt.values
    corr = sc_stats.pearsonr(
        ar_trend_max.isel(ardt=i).values, ann_ar_precip.isel(ardt=i).values
    )
    res.append([ardt, corr.statistic, corr.pvalue])
```

```{python}
corr_df = pd.DataFrame(res, columns=["ardt", "r2", "p-val"])
corr_df
```

```{python}
res = []
for i in range(4):
    ardt = ar_trend_max.isel(ardt=i).ardt.values
    corr = sc_stats.pearsonr(
        ar_trend_max.isel(ardt=i).values, ann_ar_precip_max.isel(ardt=i).values
    )
    res.append([ardt, corr.statistic, corr.pvalue])
```

```{python}
corr_df = pd.DataFrame(res, columns=["ardt", "r2", "p-val"])
corr_df
```

Correlation between AR precip and total precip.

```{python}
res = []
for i in range(4):
    ardt = ar_trend_max.isel(ardt=i).ardt.values
    corr = sc_stats.pearsonr(ann_precip.values, ann_ar_precip_max.isel(ardt=i).values)
    res.append([ardt, corr.statistic, corr.pvalue])
```

```{python}
corr_df = pd.DataFrame(res, columns=["ardt", "r2", "p-val"])
corr_df
```

```{python}
res = []
for i in range(4):
    ardt = ar_trend_max.isel(ardt=i).ardt.values
    corr = sc_stats.pearsonr(
        ann_precip_max.values, ann_ar_precip_max.isel(ardt=i).values
    )
    res.append([ardt, corr.statistic, corr.pvalue])
```

```{python}
corr_df = pd.DataFrame(res, columns=["ardt", "r2", "p-val"])
corr_df
```

Figure.

```{python}
fig, ax = plt.subplots(nrows=4, figwidth="12cm", figheight="13cm", abc=True)
bar_ax = ax.alty(color="C0")

bar_colors = plt.get_colors("tab20c")[:4]
for i in range(4):
    (ar_trend.isel(ardt=i) * 100).plot(
        ax=ax[i], label="AR spatial avg.", zorder=5, c="C1", lw=1.3
    )
    (ar_trend_max.isel(ardt=i) * 100).plot(
        ax=ax[i], zorder=5, c="C1", ls="--", label="AR spatial P$_{99}$", lw=1.3
    )

    annotation = f"{ar_trend.isel(ardt=i).ardt.values}"
    ax[i].format(title=annotation)

    bar_ax[i].bar(
        ann_precip.year,
        ann_precip_max.values,
        label="TP spatial P$_{99}$",
        c=bar_colors[1],
    )
    bar_ax[i].bar(
        ann_precip.year,
        ann_ar_precip_max.isel(ardt=i).values,
        label="AR TP spatial P$_{99}$",
        c=bar_colors[0],
    )

    # bar_ax[i].bar(
    #     ann_precip.year,
    #     ann_precip.isel(ardt=i).values,
    #     label="AR TP spatial avg.",
    #     c=bar_colors[0],
    # )

handles, labels = ax[-1].get_legend_handles_labels()
bar_handles, bar_labels = bar_ax[-1].get_legend_handles_labels()

handles.extend(bar_handles)
labels.extend(bar_labels)

fig.legend(handles, labels, loc="b", ncols=2)
# bar_ax.format(ylabel="AR Total Precipitation [m]")
fig.text(
    0.985,
    0.55,
    "Total Precipitation [m]",
    va="center",
    ha="center",
    rotation="vertical",
    in_layout=True,
    color="C0",
)
bar_ax[2].format(
    ylabel=" ",
)

ax.format(
    # color="C1",
    suptitle="Annual AR frequency and total precipitation",
    xlabel="Year",
    ylabel="AR Frequency [%]",
    # title=ar_trend.ardt.values,
    abcloc="ul",
    abcbbox=True,
    abc_kw={"bbox": {"boxstyle": "square", "facecolor": "white"}},
)
```

```{python}
fig.savefig("../../figures/ar_ann_avg_trends_alt.svg")
```

```{python}
ann_tp_tot = (
    precip_ds.groupby("valid_time.year")
    .sum()
    .sum(["latitude", "longitude"])
    .tp.compute()
)
```

```{python}
weights = np.cos(np.deg2rad(ar_ens_ds.lat))
ar_trend_tot = (
    ar_ens_ds.groupby(["start_time.year", "ardt"])
    .sum()
    .weighted(weights)
    .sum(dim=["lat", "lon"])
    .ar_tracked_id
)  # / n_days.values.reshape((-1, 1))
ar_trend_tot = ar_trend_tot.compute()
```

### Trends in number of AR timesteps

```{python}
def get_unique(ds):
    unique_shape = np.unique(ds).shape[0]
    res = unique_shape / ds.shape[0]
    return xr.DataArray(res)
```

```{python}
unique = ar_ts_ens.valid_time.groupby("ardt").map(get_unique)
```

```{python}
unique.to_pandas().median()
```

```{python}
ar_ts_trend = ar_ts_ens.groupby(["valid_time.year", "ardt"]).count()  # / n_days * 100
```

```{python}
mk_res_avg = []
for ardt_ts in ar_ts_trend.values.T:
    results = mk.original_test(ardt_ts)
    mk_res_avg.append([results.p, results.slope])
```

```{python}
np.median(np.asarray(mk_res_avg), axis=0)
```

```{python}
bar_colors = plt.get_colors("tab20c")[:4]
fig, ax = plt.subplots(figheight="6cm", figwidth="12cm")

# for i in range(4):
#     ar_ts_trend.isel(ardt=i).plot(ax=ax, label=ar_ts_trend.isel(ardt=i).ardt, zorder=5)
ar_ts_trend.median("ardt").plot(ax=ax, label="ARDT median", c="k", lw=1, zorder=5)

ar_ts_trend.min("ardt").plot(
    ax=ax, label="ARDT min-max", c="k", lw=1, ls="--", zorder=5, alpha=0.6
)
ar_ts_trend.max("ardt").plot(ax=ax, c="k", lw=1, ls="--", zorder=5, alpha=0.6)

# ax.fill_between(ar_ts_trend.year, trend_min, trend_max, zorder=5, alpha=0.5)

bar_ax = ax.twinx(color="C0", ylabel="Precipitation [m]")


bar_ax.bar(
    ann_precip.year, ann_precip.values, c=bar_colors[1], label="Annual precipitation"
)
bar_ax.bar(
    ann_precip.year,
    ann_ar_precip.median("ardt").values,
    c=bar_colors[0],
    label="Annual AR precipitation",
)
ax.legend(loc="b", ncols=4)
ax.format(
    suptitle="Total annual AR occurrence and average annual precipitation over Scandinavia",
    title="",
    xlabel="Year",
    ylabel="AR occurrence",
)
```

```{python}
fig.savefig("../../figures/ar_total_frq_tp.svg")
```

```{python}
res = []
for i in range(4):
    ardt = ar_ts_trend.isel(ardt=i)
    corr = sc_stats.pearsonr(ann_precip.values, ardt.values)
    res.append([ardt.ardt.values, corr.statistic, corr.pvalue])
```

```{python}
corr_df = pd.DataFrame(res, columns=["ardt", "r2", "p-val"])
corr_df
```

```{python}
corr_df[["r2", "p-val"]].median(axis=0)
```

```{python}
corr_df.to_csv("/data/projects/atmo_rivers_scandinavia/ar_total_frq_corr.csv")
```

```{python}
res = []
for i in range(4):
    ardt = ar_ts_trend.isel(ardt=i)
    corr = sc_stats.pearsonr(ann_ar_precip.isel(ardt=i).values, ardt.values)
    res.append([ardt.ardt.values, corr.statistic, corr.pvalue])
```

```{python}
corr_df = pd.DataFrame(res, columns=["ardt", "r2", "p-val"])
corr_df
```

```{python}
corr_df[["r2", "p-val"]].median(axis=0)
```

```{python}
corr_df.to_csv("/data/projects/atmo_rivers_scandinavia/ar_total_frq_corr_ar_tp.csv")
```

```{python}
res = []
for i in range(4):
    ardt = ar_ts_trend.isel(ardt=i)
    corr = sc_stats.pearsonr(ann_ar_precip.isel(ardt=i).values, ann_precip.values)
    res.append([ardt.ardt.values, corr.statistic, corr.pvalue])
```

```{python}
corr_df = pd.DataFrame(res, columns=["ardt", "r2", "p-val"])
corr_df
```

```{python}
corr_df[["r2", "p-val"]].median(axis=0)
```

```{python}
corr_df.to_csv("/data/projects/atmo_rivers_scandinavia/corr_ar_precip_ann_precip.csv")
```

## Spatial correlation over time

```{python}
ann_tp_field = precip_ds.groupby("valid_time.year").sum().tp.compute()
```

```{python}
ann_ar_ens_field = (
    ar_ens_ds.groupby(["start_time.year", "ardt"]).sum().ar_tracked_id.compute()
)
```

```{python}
n_years = ann_tp_field.year.shape[0]
n_ardts = 4
```

```{python}
spatial_correlations = np.zeros((n_years, n_ardts))
for year in range(n_years):
    for ardt_idx in range(n_ardts):
        spatial_correlations[year, ardt_idx] = compute_spatial_correltion(
            ann_ar_ens_field.isel(ardt=ardt_idx, year=year),
            ann_tp_field.isel(year=year),
        )
```

```{python}
fig, ax = plt.subplots()
ax.plot(ann_tp_field.year, spatial_correlations.mean(axis=1))
ax.fill_between(
    ann_tp_field.year,
    spatial_correlations.min(axis=1),
    spatial_correlations.max(axis=1),
    alpha=0.7,
)
```

```{python}
ann_ar_ens_field_norm = ann_ar_ens_field / (
    ann_ar_ens_field.max() - ann_ar_ens_field.min()
)
ann_tp_field_norm = ann_tp_field / (ann_tp_field.max() - ann_tp_field.min())
```

```{python}
fig, ax = plt.subplots()
ann_ar_ens_field_norm.isel(ardt=1).mean("lon").plot(ax=ax)
```

```{python}
fig, ax = plt.subplots()
ann_tp_field_norm.cf.mean("longitude").T.plot(ax=ax)
```

## Average timestep

Get histograms.
Select only unique time steps, no doubles.

Here we compute the spatial average and 75th percentile before doing the histogram.
- Reduces the variability somewhat.

```{python}
ar_precip_ens = ar_precip_ens.compute()
n_ar_precip_ens = n_ar_precip_ens.compute()

ar_precip_ens_nz = ar_precip_ens  # .where(ar_precip_ens.tp > 0, drop=True)
n_ar_precip_ens_nz = n_ar_precip_ens  # .where(n_ar_precip_ens.tp > 0, drop=True)
```

```{python}
weights = np.cos(np.deg2rad(ar_precip_ens_nz.latitude))

# Both of these reductions are weighted.
# Mean
ar_precip_hist = (ar_precip_ens_nz).weighted(weights).mean(["latitude", "longitude"])
n_ar_precip_hist = (
    (n_ar_precip_ens_nz).weighted(weights).mean(["latitude", "longitude"])
)

# Max (0.75 quantile)
# No weights for quantiles.
ar_precip_hist_max = (
    (ar_precip_ens_nz).weighted(weights).quantile(1, dim=["latitude", "longitude"])
)
n_ar_precip_hist_max = (
    (n_ar_precip_ens_nz).weighted(weights).quantile(1, dim=["latitude", "longitude"])
)
```

Compute the histograms.

```{python}
bin_threshold_qtile = 0.99
density = True
n_bins = 10
```

```{python}
bin_threshold = np.round(
    max(
        ar_precip_hist.tp.quantile(bin_threshold_qtile).compute(),
        n_ar_precip_hist.tp.quantile(bin_threshold_qtile).compute(),
    ),
    decimals=2,
)
bin_max = np.round(
    max(
        ar_precip_hist.tp.max().compute(),
        n_ar_precip_hist.tp.max().compute(),
    ),
    decimals=2,
)
```

Get the bin range for the histograms.

```{python}
# hist_bin_range_avg = (0, bin_max.values)
bins_avg = np.linspace(0, bin_threshold.values, n_bins + 1)
bins_avg_open = bins_avg.copy()
bins_avg_open[-1] = bin_max.values
```

Histograms for the spatial average.

```{python}
hist_ar = np.zeros((4, n_bins))
for i, ardt in enumerate(ARDT_NAMES):
    indices = ar_precip_hist.tp.groupby("ardt").groups[ardt]
    hist, _ = np.histogram(
        ar_precip_hist.isel(valid_time=indices).tp.data,
        # range=hist_bin_range_avg,
        bins=bins_avg_open,
        density=density,
    )
    hist_ar[i] = hist
```

```{python}
hist_n_ar, _ = np.histogram(
    n_ar_precip_hist.tp.data, bins=bins_avg_open, density=density
)
# hist_n_ar = hist_n_ar.compute()
```

```{python}
bins_avg_open
```

Histograms for the spatial max.

```{python}
bin_threshold = np.round(
    max(
        ar_precip_hist_max.tp.quantile(bin_threshold_qtile).compute(),
        n_ar_precip_hist_max.tp.quantile(bin_threshold_qtile).compute(),
    ),
    decimals=2,
)

bin_max = np.round(
    max(
        ar_precip_hist_max.tp.max().compute(),
        n_ar_precip_hist_max.tp.max().compute(),
    ),
    decimals=2,
)
```

Get the bin range for the histograms.

```{python}
bins_max = np.linspace(0, bin_threshold.values, n_bins + 1)
bins_max_open = bins_max.copy()
bins_max_open[-1] = bin_max.values
```

```{python}
qtile_hist_n_ar, _ = np.histogram(
    n_ar_precip_hist_max.tp.data, bins=bins_max_open, density=density
)
```

```{python}
qtile_hist_ar = np.zeros((4, n_bins))
for i, ardt in enumerate(ARDT_NAMES):
    indices = ar_precip_hist_max.tp.groupby("ardt").groups[ardt]
    hist, _ = np.histogram(
        ar_precip_hist_max.isel(valid_time=indices).tp.data,
        bins=bins_max_open,
        density=density,
    )
    qtile_hist_ar[i] = hist
```

Compute the field for the average time step with and without ARs.

```{python}
# ar_avg_pr_ts = ar_precip_ens_nz.groupby("ardt").mean().median("ardt").load()
# ar_avg_pr_ts = ar_precip_mh.where(ar_precip_mh.tp > 0).groupby("ardt").mean().mean("ardt").compute()
# n_ar_avg_pr_ts = n_ar_precip_ens_nz.cf.mean("time").compute()
```

New figure with histograms only.

```{python}
fig, ax = plt.subplots(
    figwidth="8.5cm",
    figheight="12cm",
    nrows=2,
    # figheight="12cm",
    abc=True,
    sharex=False,
)


# ax.stairs(np.max(hist_ar, axis=0), bins, color="C0", fill=True, alpha=0.5, zorder=0)
# ax.fill_between(
#    bins[1:],
#    np.min(hist_ar, axis=0),
#    np.max(hist_ar, axis=0),
#    color="C0",
#    step="pre",
#    alpha=0.2,
# )
qtile = 0  # 0.975

ens_min = np.quantile(hist_ar, qtile, axis=0)
ens_min = np.concat([ens_min[:1], ens_min])

ens_max = np.quantile(hist_ar, 1 - qtile, axis=0)
ens_max = np.concat([ens_max[:1], ens_max])

ens_med = np.median(hist_ar, axis=0)
ens_med_pad = np.concat([ens_med[:1], ens_med])

ax[0].fill_between(
    bins_avg,
    ens_min,
    ens_max,
    color="C0",
    step="pre",
    alpha=0.2,
)

# Spatial average
ax[0].stairs(ens_med, bins_avg, label="AR [Median]", color="C0")
## Non AR conditions.
ax[0].stairs(hist_n_ar, bins_avg, label="No AR", color="C1")

# Spatial max
ens_min = np.quantile(qtile_hist_ar, qtile, axis=0)
ens_min = np.concat([ens_min[:1], ens_min])

ens_max = np.quantile(qtile_hist_ar, 1 - qtile, axis=0)
ens_max = np.concat([ens_max[:1], ens_max])

ens_med = np.median(qtile_hist_ar, axis=0)
ax[1].stairs(ens_med, bins_max, color="C0")
## Non AR conditions.
ax[1].stairs(qtile_hist_n_ar, bins_max, color="C1")

ax[1].fill_between(
    bins_max,
    ens_min,
    ens_max,
    color="C0",
    step="pre",
    alpha=0.2,
)

# ax.fill_between(
#     bins,
#     np.concat([hist_n_ar[:1], hist_n_ar]),
#     np.concat([qtile_hist_n_ar[:1], qtile_hist_n_ar]),
#     color="C1",
#     step="pre",
#     alpha=0.2,
# )


ax.format(
    yscale="log",
    xlabel="Precipitation [mm/6h]",
    ylabel="Density",
    ytickloc="l",
    ylabelloc="l",
    title="Histogram of 1 hour precipitation",
)
fig.legend(loc="b", ncols=3)

ax.format(
    title="",
    suptitle="Histograms of 6-hourly total precipitation",
    rowlabels=["Spatial average", "Spatial max"],
    abcloc="ur",
    abcbbox=True,
    abc_kw={"bbox": {"boxstyle": "square", "facecolor": "white"}},
)
```

```{python}
fig.savefig("../../figures/ar_timestep_hist.svg")
```

What is the difference between the average AR rain rate and the non-AR rain rate?

### Tests

```{python}
test = (
    ar_precip_ens.groupby("ardt").mean().load()  # where(ar_precip_ens_unique.tp > 0)
    / 6
)
```

```{python}
fig, ax = pplt.subplots(ncols=4)
for i in range(4):
    test.isel(ardt=i).tp.plot(ax=ax[i], vmin=0, discrete=False)
```

```{python}
test.mean("ardt").tp.plot(vmin=0)
```

### Time step std.

```{python}
# mean
ar_std = ar_precip_ens_nz.groupby("ardt").mean().std("ardt").compute() / 6
```

```{python}
fig, axs = pplt.subplots(
    figwidth="8.3cm",
    # figheight="12cm",
    nrows=2,
    proj=2 * ["lcc"],
    proj_kw={"central_longitude": 15},
    sharey=False,
    sharex=False,
    abc=True,
)


cmap = pplt.Colormap("oslo_r", right=0.8)
qtile = 1  # 0.95
# vmax = max(ar_avg_pr_ts.tp.quantile(qtile).values, ar_std.tp.quantile(qtile).values)
pr_cm = (ar_avg_pr_ts.tp).plot(
    ax=axs[0],
    cmap=cmap,
    vmin=0,
    # vmax=vmax,
    add_colorbar=False,
    rasterized=True,
)
pr_cbar = axs[0].colorbar(pr_cm, loc="right", label="Precipitation [mm/h]")

pr_cm = (ar_std.tp).plot(
    ax=axs[1],
    cmap=cmap,
    vmin=0,
    # vmax=vmax,
    add_colorbar=False,
    rasterized=True,
)
pr_cbar = axs[1].colorbar(pr_cm, loc="right", label="Precipitation [mm/h]")

axs.format(
    coast=True,
    reso="med",
    lonlim=LON_SLICE,
    latlim=LAT_SLICE,
    lonlabels=True,
    latlabels=True,
    title="",
    rowlabels=[
        "Avg",
        "Std.",
    ],
    suptitle="Average hourly AR precipitation [Ensemble avg. & std.]",
    abcloc="ul",
    abcbbox=True,
    abc_kw={"bbox": {"boxstyle": "square", "facecolor": "white"}},
)
```

```{python}
fig.savefig("../../figures/avg_std_precip_timestep_ar_ens.svg")
```

# Cluster ensemble

Could we have a start and end time coordinate for the samples? In order to be able to index by time?

```{python}
dedup = []
for name in ARDT_NAMES:
    dedup.append(
        ar_ts_ens.where(ar_ts_ens.ardt == name, drop=True).drop_duplicates("valid_time")
    )
ar_ts_ens = xr.concat(dedup, dim="valid_time")
```

```{python}
ar_precip_ens = ar_precip_ens.assign_coords({"label": ar_ts_ens})
```

```{python}
(
    ar_precip_ens.valid_time.groupby(["valid_time.year", "ardt", "label"]).count()
    / n_days
    * 100
).mean("year")
```

```{python}
# NOTE: Need to divide by 4 since we have 4 ARDTs?
ar_precip_ens_avg = (
    ar_precip_ens.groupby(["label", "ardt", "valid_time.year"])
    .sum()
    .mean("year")
    # .median("ardt")
).load()
ar_precip_ens_avg_ts = ar_precip_ens.groupby(["label"]).mean().load()
```

```{python}
n_days_groups = n_days.values.reshape((1, 1, -1, 1, 1))
```

```{python}
grouped_ars = (
    (
        ar_ens_ds.groupby(["start_time.year", "label", "ardt"]).sum() / n_days_groups
    ).mean(["year"])
    # .median("ardt")
)
```

```{python}
grouped_ars = grouped_ars.load()
```

## Annual average and total precipitation fraction
First compute the correlations.

```{python}
def get_cluster_correlations(ar_freqs, ar_precip):
    correlations = []
    for label in range(grouped_ars.label.shape[0]):
        corr = compute_spatial_correltion(
            ar_freqs.isel(label=label).ar_tracked_id,
            ar_precip.isel(label=label).tp / ann_avg_precip_ds,
            dim=["lat", "lon"],
        )
        correlations.append(corr)
    correlations = xr.concat(correlations, dim="label")
    return correlations
```

```{python}
correlations = get_cluster_correlations(grouped_ars, ar_precip_ens_avg)
```

```{python}
correlations = correlations.quantile([0, 0.5, 1], "ardt").to_pandas()
# And save them to disk.
correlations.to_csv(
    "/data/projects/atmo_rivers_scandinavia/ensemble_correlations_per_cluster.csv"
)
```

```{python}
correlations
```

```{python}
def ar_cluster_plot(ar_freqs, ar_pr, suptitle):
    n_clusters = grouped_ars.label.shape[0]
    fig, axs = plt.subplots(
        figwidth="12cm",
        nrows=n_clusters,
        ncols=2,
        proj=n_clusters * 2 * ["lcc"],
        proj_kw={"central_longitude": 15},
        share=False,
        abc=True,
    )
    vmax = ar_precip_ens_avg.tp.max().values
    cmap = plt.Colormap("oslo_r", right=0.8)
    for i in range(n_clusters):
        cm = (ar_freqs.isel(label=i).ar_tracked_id * 100).plot(
            ax=axs[i, 0],
            vmin=0,
            rasterized=True,
            add_colorbar=False,
            cmap="bamako",
        )
        axs[i, 0].colorbar(
            cm,
            label="Frequency [%]",
            width=0.15,
            # locator="multiple",
            # locator_kw={"base": 0.5},
        )
        curr_tp = ar_pr.isel(label=i).tp
        cm = (curr_tp / ann_avg_precip_ds * 100).plot(
            ax=axs[i, 1],
            vmin=0,
            # vmax=16,
            cmap=cmap,
            rasterized=True,
            add_colorbar=False,
            # cbar_kwargs={"label": "Total precipitation\nfraction [%]"},
        )
        axs[i, 1].colorbar(cm, label="Total precipitation\nfraction [%]", width=0.15)
        axs[i, 0].format(latlabels=True)
        if i == 3:
            axs[i, 0].format(latlabels=True, lonlabels=True)
            axs[i, 1].format(lonlabels=True)

    axs.format(
        coast=True,
        reso="med",
        lonlim=LON_SLICE,
        latlim=LAT_SLICE,
        title="",
        abcloc="ul",
        abcbbox=True,
        abc_kw={"bbox": {"boxstyle": "square", "facecolor": "white"}},
        rowlabels=[f"Cluster {i}" for i in range(1, n_clusters + 1)],
        collabels=["AR frequency", "Total precipitation fraction"],
        suptitle=suptitle,
    )
    return fig
```

```{python}
fig = ar_cluster_plot(
    grouped_ars.median("ardt"),
    ar_precip_ens_avg.median("ardt"),
    "AR clusters [Ensemble median]",
)
```

```{python}
fig.savefig("../../figures/ann_avg_ar_pr_clusters_ens_med.svg")
```

### Tests

```{python}
test = (
    ar_ens_ds.groupby(["start_time.year", "label", "ardt"]).sum().mean(["year"])
).load()
```

Here are individual frequencies for clusters and ardts.
The mean of a column should match the average for a cluster above.

```{python}
fig, ax = pplt.subplots(ncols=4, nrows=4)
for i in range(4):
    for j in range(4):
        (test.ar_tracked_id.isel(ardt=j, label=i) / 14.64).plot(ax=ax[i, j])
```

```{python}
test = test.sum("label")
```

We expect these plots to roughly match what we've seen in the tests for the annual averages.

```{python}
fig, ax = pplt.subplots(ncols=4)
for i in range(4):
    (test.ar_tracked_id.isel(ardt=i) / 14.64).plot(ax=ax[i])
```

Checking total precipitation estimates.

```{python}
tp_test = ar_precip_ens.where(ar_precip_ens.ardt == "Mundhenk_v3", drop=True)
```

Assumed that dates only appear once in a label?
That this assert-statement fails indicates otherwise.

```{python}
assert (
    tp_test.valid_time.shape[0]
    == tp_test.drop_duplicates("valid_time").valid_time.shape[0]
)
```

```{python}
tp_test = (
    tp_test.drop_duplicates("valid_time")
    .cf.groupby(["time.year", "label"])
    .sum()
    .mean("year")
    .load()
)
```

These percentages should be in the ballpark of the ARDT mean above?

```{python}
fig, ax = pplt.subplots(ncols=4)
cmap = pplt.Colormap("oslo_r", right=0.8)
for i in range(4):
    (tp_test.isel(label=i).tp / ann_avg_precip_ds * 100).plot(
        ax=ax[i], vmin=0, cmap=cmap
    )
```

This should be equal the corresponding test for [AR precipitation](#AR-precipitation)

```{python}
(tp_test.sum("label").tp / ann_avg_precip_ds * 100).plot()
```

### Spread

```{python}
fig = ar_cluster_plot(
    grouped_ars.max("ardt"),
    ar_precip_ens_avg.max("ardt"),
    "AR clusters [Ensemble max]",
)
```

```{python}
fig.savefig("../../figures/ann_avg_ar_pr_clusters_ens_max.svg")
```

```{python}
fig = ar_cluster_plot(
    grouped_ars.min("ardt"),
    ar_precip_ens_avg.min("ardt"),
    "AR clusters [Ensemble min]",
)
```

```{python}
fig.savefig("../../figures/ann_avg_ar_pr_clusters_ens_min.svg")
```

### Trends

```{python}
weights = np.cos(np.deg2rad(ar_ens_ds.lat))
group_ar_trends = (
    (ar_ens_ds.groupby(["start_time.year", "label", "ardt"]).sum() / n_days_groups)
    .weighted(weights)
    .mean(["lat", "lon"])
).compute()

group_ar_trends_ext = (
    (ar_ens_ds.groupby(["start_time.year", "label", "ardt"]).sum() / n_days_groups)
    .weighted(weights)
    .quantile(0.99, dim=["lat", "lon"])
).compute()
```

```{python}
group_ar_trends = group_ar_trends.fillna(0)
group_ar_trends_ext = group_ar_trends_ext.fillna(0)
```

```{python}
n_clusters = grouped_ars.label.shape[0]
avg_res = np.zeros((n_clusters, 4))
ext_res = np.zeros((n_clusters, 4))
for label in range(n_clusters):
    for ardt in range(4):
        data = group_ar_trends.isel(ardt=ardt, label=label).ar_tracked_id.values
        results = mk.original_test(data).p
        avg_res[label, ardt] = results

        data = group_ar_trends_ext.isel(ardt=ardt, label=label).ar_tracked_id.values
        results = mk.original_test(data).p
        ext_res[label, ardt] = results
```

```{python}
fig, axs = plt.subplots(
    figwidth="12cm",
    figheight="18cm",
    nrows=n_clusters,
    ncols=2,
    # sharey=False,
    # sharex=False,
    abc=True,
)
for label in range(n_clusters):
    for ardt in range(4):
        (group_ar_trends.isel(ardt=ardt, label=label).ar_tracked_id * 100).plot(
            ax=axs[label, 0],
            label=f"p: {avg_res[label, ardt]:.2f}",
        )
        (group_ar_trends_ext.isel(ardt=ardt, label=label).ar_tracked_id * 100).plot(
            ax=axs[label, 1],
            label=f"p: {ext_res[label, ardt]:.2f}",
        )
        axs[label, 0].legend(loc="ur", ncols=2)
        axs[label, 1].legend(loc="ur", ncols=2)

handles, labels = axs[0, 0].get_legend_handles_labels()
fig.legend(handles, group_ar_trends.ardt.values, loc="b", ncols=4)
axs.format(
    title="",
    abcloc="ul",
    abcbbox=True,
    abc_kw={"bbox": {"boxstyle": "square", "facecolor": "white"}},
    rowlabels=[f"Cluster {i}" for i in range(1, n_clusters + 1)],
    collabels=["Spatial average", "Spatial extreme"],
    suptitle="AR cluster frequency trends",
    # ylim=(-0.2, 12),
    xlabel="Year",
    ylabel="Ann. avg. frequency [%]",
)
```

```{python}
fig.savefig("../../figures/ar_ann_avg_trends_per_cluster.svg")
```

```{python}
fig, axs = plt.subplots(
    figwidth="8.5cm",
    figheight="15cm",
    nrows=n_clusters,
    ncols=1,
    # sharey=False,
    # sharex=False,
    abc=True,
)
for label in range(n_clusters):
    (group_ar_trends.isel(label=label).median("ardt").ar_tracked_id * 100).plot(
        ax=axs[label],
        label=f"p: {np.median(avg_res[label]):.2f}",
    )
    (group_ar_trends_ext.isel(label=label).median("ardt").ar_tracked_id * 100).plot(
        ax=axs[label],
        label=f"p: {np.median(ext_res[label]):.2f}",
    )
    # Trends ensemble min/max.
    trend_min = group_ar_trends.isel(label=label).min("ardt").ar_tracked_id * 100
    trend_max = group_ar_trends_ext.isel(label=label).max("ardt").ar_tracked_id * 100
    axs[label].fill_between(
        trend_min.year, trend_min, trend_max, alpha=0.4, c="gray", label=""
    )

    axs[label, 0].legend(loc="ur", ncols=2)
    axs[label, 0].legend(loc="ur", ncols=2)

handles, labels = axs[0, 0].get_legend_handles_labels()
fig.legend(handles, ["Spatial Avgerage", "Spatial Extreme"], loc="b", ncols=4)
axs.format(
    title="",
    abcloc="ul",
    abcbbox=True,
    abc_kw={"bbox": {"boxstyle": "square", "facecolor": "white"}},
    rowlabels=[f"Cluster {i}" for i in range(1, n_clusters + 1)],
    suptitle="AR cluster frequency trends",
    # ylim=(-0.2, 12),
    xlabel="Year",
    ylabel="Ann. avg. frequency [%]",
)
```

```{python}
fig.savefig("../../figures/ar_ann_avg_trends_per_cluster_v2.svg")
```

### Cluster seasonal cycles
We group by month, sum the AR occurrences, and then divide by the number of time steps in each month.
No yearly grouping.

```{python}
n_ts_per_month = precip_ds.valid_time.groupby(
    ["valid_time.month", "valid_time.year"]
).count()
```

```{python}
weights = np.cos(np.deg2rad(ar_ens_ds.lat))
group_ar_cycle = (
    (ar_ens_ds.groupby(["start_time.month", "label", "ardt"]).sum()) / n_ts_per_month
).compute()
```

```{python}
group_ar_cycle = ar_precip_ens.valid_time.groupby(
    ["valid_time.year", "valid_time.month", "label", "ardt"]
).count()
group_ar_yr = ar_precip_ens.valid_time.groupby(
    ["valid_time.year", "label", "ardt"]
).count()
```

Compute the weighted spatial 75th percentile.

```{python}
# data = (group_ar_cycle).weighted(weights).quantile(0.75, dim=["lat", "lon"])
# data = group_ar_cycle.mean("year")
data = (group_ar_cycle / n_ts_per_month) * 100
```

```{python}
data = data.roll({"month": 1}, roll_coords=False)  # * 100
```

```{python}
n_clusters = 4
fig, ax = plt.subplots(nrows=4, figwidth="8.5cm", figheight="12cm", abc=True)
win_size = 10
# Isn't this just spatial mean?
for i in range(4):
    (data.isel(label=i).median("ardt").mean("year")).plot(
        ax=ax[i], label="Ensemble Median", lw=1
    )
    cycle_max = data.isel(label=i).max("ardt").mean("year")  # .ar_tracked_id
    cycle_min = data.isel(label=i).min("ardt").mean("year")  # .ar_tracked_id
    ax[i].fill_between(
        cycle_min.month, cycle_min, cycle_max, alpha=0.5, label="Ensemble Min - Max"
    )

handles, labels = ax[i].get_legend_handles_labels()
fig.legend(handles, labels, loc="b", ncols=2)
months = [
    "Dec",
    "Jan",
    "Feb",
    "Mar",
    "Apr",
    "May",
    "Jun",
    "Jul",
    "Aug",
    "Sep",
    "Oct",
    "Nov",
]
ax.format(
    title="",
    xlabel="Month",
    ylabel="Total AR frequency [%]",
    rowlabels=[f"Cluster {i}" for i in range(1, n_clusters + 1)],
    suptitle="Average AR seasonal cycle",
    xlocator=range(1, 13),
    # xtickminor=False,
    xticklabels=months,
    # xformatter="%b",
    xrotation=0,
    ymin=0,
    ymax=25,
    abcloc="ul",
    abcbbox=True,
    abc_kw={"bbox": {"boxstyle": "square", "facecolor": "white"}},
)
```

```{python}
fig.savefig("../../figures/avg_seasonal_cycle_per_cluster.svg")
```

```{python}
n_clusters = 4
fig, ax = plt.subplots(nrows=4, figwidth="8.5cm", figheight="12cm", abc=True)
win_size = 10
# Isn't this just spatial mean?
for i in range(4):
    (data.isel(label=i).median("ardt").mean("year")).plot(
        ax=ax[i], label="Ensemble Median\nAverage year", lw=1
    )
    cycle_max = (
        data.isel(label=i).median("ardt").quantile(0.75, "year")
    )  # .ar_tracked_id
    cycle_min = (
        data.isel(label=i).median("ardt").quantile(0.25, "year")
    )  # .ar_tracked_id
    ax[i].fill_between(
        cycle_min.month,
        cycle_min,
        cycle_max,
        alpha=0.5,
        label="Ensemble Median\nAnnual IQR",
    )

handles, labels = ax[i].get_legend_handles_labels()
fig.legend(handles, labels, loc="b", ncols=2)
months = [
    "Dec",
    "Jan",
    "Feb",
    "Mar",
    "Apr",
    "May",
    "Jun",
    "Jul",
    "Aug",
    "Sep",
    "Oct",
    "Nov",
]
ax.format(
    title="",
    xlabel="Month",
    ylabel="Total AR frequency [%]",
    rowlabels=[f"Cluster {i}" for i in range(1, n_clusters + 1)],
    suptitle="Average AR seasonal cycle",
    xlocator=range(1, 13),
    # xtickminor=False,
    xticklabels=months,
    # xformatter="%b",
    xrotation=0,
    ymin=0,
    # ymax=25,
    abcloc="ul",
    abcbbox=True,
    abc_kw={"bbox": {"boxstyle": "square", "facecolor": "white"}},
)
```

This is somewhat hinting at what I suspected in the seasonality. That we don't se propely in the maps? This is better no?

## Average time step
Not sure if this is interesting?

```{python}
correlations = []
for label in range(4):
    corr = compute_spatial_correltion(
        grouped_ars.isel(label=label).ar_tracked_id / 1464,
        ar_precip_ens_avg_ts.isel(label=label).tp / 6,
    )
    correlations.append(corr)
correlations = xr.concat(correlations, dim="label")
```

```{python}
grouped_ars.isel(label=0)
```

```{python}
group_avg_ar = ar_ens_ds.groupby(["label"]).mean().load()
```

```{python}
fig, axs = pplt.subplots(
    figwidth="12cm",
    nrows=4,
    ncols=2,
    proj=4 * 2 * ["lcc"],
    proj_kw={"central_longitude": 15},
    sharey=False,
    sharex=False,
    abc=True,
)
vmax = ar_precip_ens_avg.tp.max().values
for i in range(4):
    cm = (grouped_ars.isel(label=i).ar_tracked_id / 1464 * 100).plot(
        # cm = (group_avg_ar.isel(label=i).ar_tracked_id).plot(
        ax=axs[i, 0],
        vmin=0,
        rasterized=True,
        add_colorbar=False,
        # cbar_kwargs={"label": "Frequency [%]"},
    )
    axs[i, 0].colorbar(cm, label="Frequency [%]", width=0.15)

    cm = (ar_precip_ens_avg_ts.isel(label=i).tp / 6).plot(
        ax=axs[i, 1], vmin=0, cmap="oslo_r", rasterized=True, add_colorbar=False
    )
    axs[i, 1].colorbar(cm, label="Precipitation [mm/h]", width=0.15)

    axs[i, 1].annotate(
        f"Corr: {correlations.isel(label=i).values:.2f}",
        (0.95, 0.95),
        xycoords="axes fraction",
        ha="right",
        va="top",
        bbox={"boxstyle": "square", "facecolor": "white"},
    )
    axs[i, 0].format(latlabels=True)
    if i == 3:
        axs[i, 0].format(latlabels=True, lonlabels=True)
        axs[i, 1].format(lonlabels=True)


axs.format(
    coast=True,
    reso="med",
    lonlim=LON_SLICE,
    latlim=LAT_SLICE,
    title="",
    abcloc="ul",
    abcbbox=True,
    abc_kw={"bbox": {"boxstyle": "square", "facecolor": "white"}},
    rowlabels=[f"Cluster {i}" for i in range(1, 5)],
    collabels=["AR Frequency", "Timestep avg. precipitation"],
    suptitle="Ensemble average AR clusters",
)
```

```{python}
fig.savefig("../../figures/avg_precip_ar_timestep_ens_per_cluster.svg")
```

## Clusters grouped by season

```{python}
ar_ens_ds = ar_ens_ds.load()
```

```{python}
n_days_season = precip_ds.valid_time.groupby(
    ["valid_time.year", "valid_time.season"]
).count()
```

```{python}
ann_avg = (ar_ens_ds.groupby(["start_time.year", "label", "ardt"]).sum() / n_days).mean(
    "year"
)
```

```{python}
n_days_season.shape
```

```{python}
season_grouped_ars = (
    ar_ens_ds.groupby(
        ["start_time.season", "start_time.year", "label", "ardt"]
    ).sum()  # .mean("year")
    # .median("ardt")
)
```

```{python}
season_grouped_ars = season_grouped_ars.isel(season=slice(0, -1))
```

```{python}
n_seasons = season_grouped_ars.season.shape[0]
n_years = season_grouped_ars.year.shape[0]
```

Uses broadcasting to compute frequencies per season.
`n_days_season` is the number of days in each season in each year.

```{python}
season_grouped_ars = season_grouped_ars / n_days_season.T.values.reshape(
    1, 1, n_seasons, n_years, 1, 1
)
season_grouped_ars = season_grouped_ars.mean("year")
```

Get the number of samples for each season, on average.

```{python}
season_samples = {}
for key, val in (
    precip_ds.sel(valid_time="1981").groupby("valid_time.season").groups.items()
):
    season_samples[key] = len(val)
```

```{python}
season_samples
```

Pre-compute the frequencies.

```{python}
freqs = season_grouped_ars * 100
```

```{python}
def season_cluster_plot(freqs, suptitle):
    n_clusters = freqs.label.shape[0]
    season_order = ["DJF", "MAM", "JJA", "SON"]
    nrows = n_clusters
    ncols = 4
    fig, axs = plt.subplots(
        figwidth="12cm",
        nrows=nrows,
        ncols=ncols,
        proj=nrows * ncols * ["lcc"],
        proj_kw={"central_longitude": 15},
        sharey=False,
        sharex=False,
        abc=True,
    )
    # vmax = np.round(freqs.ar_tracked_id.quantile(0.975).values, 1)

    for i in range(nrows):
        vmax = freqs.isel(label=i).ar_tracked_id.max().values
        for j, season in enumerate(season_order):
            cm = (
                freqs.isel(label=i)
                .sel(season=season)
                .ar_tracked_id.plot(
                    ax=axs[i, j],
                    rasterized=True,
                    vmin=0,
                    vmax=vmax,
                    add_colorbar=False,
                    cmap="bamako",
                )
            )

        fig.colorbar(
            cm,
            rows=(i + 1, i + 1),
            # span=(2, 3),
            label="Frequency [%]",
            width=0.1,
            # extend="max",
            loc="r",
            rasterized=True,
        )

    axs.format(
        coast=True,
        reso="med",
        lonlim=LON_SLICE,
        latlim=LAT_SLICE,
        title="",
        abcloc="ul",
        abcbbox=True,
        abc_kw={"bbox": {"boxstyle": "square", "facecolor": "white"}},
        rowlabels=[f"Cluster {i}" for i in range(1, n_clusters + 1)],
        collabels=season_order,
        suptitle=suptitle,
    )
    return fig
```

```{python}
fig = season_cluster_plot(
    freqs.median("ardt"), "Seasonal AR frequencies [Ensemble median]"
)
```

```{python}
fig.savefig("../../figures/seasonal_ar_clusters_ens_med.svg")
```

### Spread

```{python}
fig = season_cluster_plot(freqs.max("ardt"), "Seasonal AR frequencies [Ensemble max]")
```

```{python}
fig.savefig("../../figures/seasonal_ar_clusters_ens_max.svg")
```

```{python}
fig = season_cluster_plot(freqs.min("ardt"), "Seasonal AR frequencies [Ensemble min]")
```

```{python}
fig.savefig("../../figures/seasonal_ar_clusters_ens_min.svg")
```

## Cluster grouped by NAO

```{python}
def prepare_nao_ds(path: str | None = None) -> pd.Series:
    """Prepare NAO dataframe."""
    if path is None:
        path = Path(__file__).parent / "../etc/norm_daily_nao_index_1950_2024.txt"

    nao_df: pd.DataFrame = pd.read_csv(
        filepath_or_buffer=path,
        sep=r"\s+",
        header=None,
        names=["year", "month", "day", "nao"],
        dtype={"year": str, "month": str, "day": str, "nao": float},
        na_values="-99.0",
    )
    nao_df["time"] = pd.to_datetime(
        nao_df["year"] + "-" + nao_df["month"] + "-" + nao_df["day"]
    )
    nao_df.index = pd.Index(nao_df.time)
    nao_series = nao_df["nao"]
    nao_series = nao_series.interpolate()

    # Upsample to 6 hourly for convenience with era5 data.
    nao_series = nao_series.resample("6h").ffill()

    # first_timestep = ar_ds.time[:1].to_numpy()[0]
    # last_timestep = ar_ds.time[-1:].to_numpy()[0]

    # # What year do we need?
    # nao_series = nao_series.loc[first_timestep:last_timestep]

    return nao_series


def get_nao_bins(nao_series: pd.Series, midpoint_qtile: float = 0.5) -> np.ndarray:
    """Generate NAO bins with roughly equal number of samples on either side of 0."""
    # TODO: Don't hard code midpoint.
    bins = np.quantile(
        nao_series,
        [
            0,
            midpoint_qtile / 2,
            midpoint_qtile,
            midpoint_qtile + (1 - midpoint_qtile) / 2,
            1,
        ],
    )
    return bins
```

```{python}
nao_series = prepare_nao_ds(path="../etc/norm_daily_nao_index_1950_2024.txt")
nao_series = nao_series.loc["1980":"2019"]
nao_da = xr.DataArray(nao_series)
# nao_bins = get_nao_bins(nao_series)
nao_bins = (-3.5, -0.5, 0, 0.5, 3.5)

hist, bins = np.histogram(nao_series, nao_bins)
```

NAO seasonal plot.

```{python}
fig, ax = plt.subplots(figwidth="8.3cm")
test = nao_series.resample("D").mean()
test.groupby(test.index.map(lambda t: t.dayofyear)).mean().plot(ax=ax, label="Mean")
q75 = test.groupby(test.index.map(lambda t: t.dayofyear)).quantile(0.75)
q25 = test.groupby(test.index.map(lambda t: t.dayofyear)).quantile(0.25)
ax.fill_between(q75.index, q75, q25, alpha=0.5, label="25-75 percentile")
ax.legend()
ax.format(xlabel="Day of year", ylabel="NAO", suptitle="Annual average NAO [1980-2019]")
```

```{python}
fig.savefig("../../figures/nao_season.svg")
```

Select the nao values for dates when we have ARs

```{python}
ar_nao_values = nao_da.sel(time=ar_ens_ds.start_time, method="nearest")
ar_nao_values = ar_nao_values.where(~ar_nao_values.label.isnull())
```

and assign them as coordinate to the ar ensemble

```{python}
ar_ens_ds = ar_ens_ds.assign_coords({"nao": ar_nao_values})
```

Groupby both nao and the cluster label.

```{python}
from xarray.groupers import BinGrouper, TimeResampler, UniqueGrouper
```

```{python}
ar_nao_groups = ar_ens_ds.groupby(
    nao=BinGrouper(nao_bins), label=UniqueGrouper(), ardt=UniqueGrouper()
).sum()
```

```{python}
# Reshape for broadcasting
nao_counts = hist.reshape((1, 1, -1, 1))
```

Calculate the frequencies

```{python}
freqs = ar_nao_groups / nao_counts * 100
```

```{python}
freqs = freqs.compute()
```

```{python}
def nao_cluster_plot(freqs, suptitle):
    n_clusters = freqs.label.shape[0]
    nrows = n_clusters
    ncols = 4
    fig, axs = plt.subplots(
        figwidth="12cm",
        nrows=nrows,
        ncols=ncols,
        proj=nrows * ncols * ["lcc"],
        proj_kw={"central_longitude": 15},
        share=False,
        abc=True,
    )

    vmax = np.round(freqs.ar_tracked_id.quantile(0.975).values, 1)
    # vmax = 3
    # print(vmax)

    for i in range(nrows):
        for j in range(4):
            data = freqs.isel(label=i, nao_bins=j).ar_tracked_id
            cm = data.plot(
                ax=axs[i, j],
                rasterized=True,
                vmin=0,
                vmax=vmax,
                add_colorbar=False,
                cmap="bamako",
            )
            local_max = data.max().values
            axs[i, j].annotate(
                f"Max: {local_max:.1f}",
                (1, 0),
                xycoords="axes fraction",
                bbox={"boxstyle": "square", "facecolor": "white"},
                ha="right",
                va="bottom",
            )
    fig.colorbar(
        cm,
        label="Frequency [%]",
        width=0.1,
        extend="max",
        loc="b",
        span=(2, 3),
        # rows=(i+1, i+1),
        rasterized=True,
    )

    axs.format(
        coast=True,
        reso="med",
        lonlim=LON_SLICE,
        latlim=LAT_SLICE,
        title="",
        abcloc="ul",
        abcbbox=True,
        abc_kw={"bbox": {"boxstyle": "square", "facecolor": "white"}},
        rowlabels=[f"Cluster {i}" for i in range(1, n_clusters + 1)],
        collabels=[f"{bin}" for bin in freqs.nao_bins.values],
        suptitle=suptitle,
    )
    return fig
```

```{python}
fig = nao_cluster_plot(
    freqs.median("ardt"), "NAO-grouped AR frequencies [Ensemble median]"
)
```

```{python}
fig.savefig("../../figures/nao_ar_clusters_ens_med.svg")
```

### Spread

```{python}
fig = nao_cluster_plot(freqs.max("ardt"), "NAO-grouped AR frequencies [Ensemble max]")
```

```{python}
fig.savefig("../../figures/nao_ar_clusters_ens_max.svg")
```

```{python}
fig = nao_cluster_plot(freqs.min("ardt"), "NAO-grouped AR frequencies [Ensemble min]")
```

```{python}
fig.savefig("../../figures/nao_ar_clusters_ens_min.svg")
```

## NAO-grouped AR frequency distributions

```{python}
import scipy.stats as scstats
```

```{python}
res = np.zeros((4, 4))
for cluster in range(4):
    for nao in range(4):
        data_curr = (
            freqs.isel(label=cluster, nao_bins=nao)
            .ar_tracked_id.median("ardt")
            .values.flatten()
        )
        res_inner = np.zeros(4)
        for nao_2 in range(4):
            data_cand = (
                freqs.isel(label=cluster, nao_bins=nao_2)
                .ar_tracked_id.median("ardt")
                .values.flatten()
            )
            res_inner[nao_2] = scstats.wasserstein_distance(data_cand, data_curr)
        res[cluster, nao] = res_inner.sum()
res
```

```{python}
def nao_dist_plot(freqs: xr.Dataset, title: str):

    res = np.zeros((4, 4))
    for cluster in range(4):
        for nao in range(4):
            data_curr = freqs.isel(
                label=cluster, nao_bins=nao
            ).ar_tracked_id.values.flatten()
            res_inner = np.zeros(4)
            for nao_2 in range(4):
                data_cand = freqs.isel(
                    label=cluster, nao_bins=nao_2
                ).ar_tracked_id.values.flatten()
                res_inner[nao_2] = scstats.wasserstein_distance(data_cand, data_curr)
            res[cluster, nao] = res_inner.sum()

    fig, ax = plt.subplots(
        figwidth="8.5cm", abc=True, sharey=False, hratios=(0.7, 0.3), nrows=2
    )

    for i in range(4):
        for j in range(4):
            data = freqs.isel(label=i, nao_bins=j).ar_tracked_id
            data = data.fillna(0)
            x = (2.5 * j) + (0.5 * i)
            parts = ax[0].violin(
                x, data.values.flatten(), fc=f"C{i}", label=f"Cluster {i+1}"
            )

    for i in range(4):
        ax[1].plot(np.arange(0.75, 9, 2.5), res[i], label=f"Cluster {i+1}")

    handles, labels = ax[0].get_legend_handles_labels()
    fig.legend(handles[::4], labels[::4], ncols=4, loc="b")
    ax[0].format(ylabel="AR frequency [%]")
    ax[1].format(ylabel="Total\nWasserstein distance")
    ax.format(
        xticks=np.arange(0.75, 9, 2.5),
        xticklabels=[f"{bin}" for bin in freqs.nao_bins.values],
        xlabel="NAO phase group",
        suptitle=title,
        abcbbox=True,
        abcloc="ul",
        abc_kw={"bbox": {"boxstyle": "square", "facecolor": "white"}},
    )
    return fig
```

```{python}
fig = nao_dist_plot(
    freqs.median("ardt"),
    title="NAO-grouped AR frequency distributions [Ensemble median]",
)
```

```{python}
fig.savefig("../../figures/nao_ar_freq_distributions_ens_med.svg")
```

```{python}
fig = nao_dist_plot(
    freqs.min("ardt"), title="NAO-grouped AR frequency distributions [Ensemble min]"
)
```

```{python}
fig.savefig("../../figures/nao_ar_freq_distributions_ens_min.svg")
```

```{python}
fig = nao_dist_plot(
    freqs.max("ardt"), title="NAO-grouped AR frequency distributions [Ensemble max]"
)
```

```{python}
fig.savefig("../../figures/nao_ar_freq_distributions_ens_max.svg")
```
