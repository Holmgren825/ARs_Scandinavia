---
title: Collapsed ARs
author: Erik Holmgren
format:
  html:
    code-fold: show
    toc: true
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.16.4
  kernelspec:
    display_name: ar_ident
    language: python
    name: ar_ident
---

```{python}
%load_ext autoreload
```

```{python}
%autoreload 2
```

```{python}
import re
from glob import glob
from itertools import combinations
from pathlib import Path

import cf_xarray
import dask.array as da
import numpy as np
import pandas as pd
import proplot as pplt
import scipy
import xarray as xr
from ar_identify.metrics import spatial_jaccard_score
from ar_scandinavia.pca_utils import combine_artmip_pca_results
from ar_scandinavia.utils import (
    compute_ar_pr_values_collapsed,
    compute_spatial_correltion,
    subsel_ds,
)
from cartopy import crs as ccrs
from dask_ml.cluster import KMeans
from distributed.client import Client
from tqdm.autonotebook import tqdm
from xr_utils.coordinates import conv_lon_to_180
```

```{python}
pplt.rc["font.size"] = 7
pplt.rc["cmap.discrete"] = False
```

```{python}
LAT_SLICE = (50, 74)
LON_SLICE = (-10, 45)
```

```{python}
client = Client()
```

```{python}
precip_path = "/data/era5/total_precipitation/total_precipitation-*6h.zarr/"
precip_ds = xr.open_mfdataset(precip_path, engine="zarr")
```

```{python}
precip_ds = precip_ds.cf.sel(time=slice("1980", "2019"))
```

```{python}
precip_ds = subsel_ds(precip_ds, LAT_SLICE, LON_SLICE)
```

```{python}
# NOTE: also convert to mm.
ann_avg_precip_ds = (
    precip_ds.tp.groupby("valid_time.year").sum().mean("year").load() * 1e3
)
ann_avg_precip_ds.attrs["units"] = "mm"
```

# Read in data

First we get the timesteps for the differnt ARDTs

```{python}
import os
```

```{python}
BASE_PATH = "/data/projects/atmo_rivers_scandinavia/"
ARDT_NAMES = ["Mundhenk_v3", "Reid500", "GuanWaliser_v2", "TempestLR"]
```

```{python}
def remap_labels(labels: da.array, label_dict: dict) -> da.array:
    return labels.map_blocks(
        lambda x: np.vectorize(label_dict.get)(x),
        dtype=int,
    )
```

```{python}
def get_cluster_timesteps(
    cluster_ds: xr.Dataset,
    precip_ds: xr.Dataset,
    ardt_name: str,
    remap_labels: bool = True,
) -> xr.Dataset:
    """Get a dataset with all timesteps for the clusters in cluster_ds."""
    curr_times = np.asarray(
        list(
            map(
                lambda x: x[-23:].split("-"),
                cluster_ds.sample_id.values,
            )
        )
    )
    labels = cluster_ds.labels.load()
    # NOTE: Here we just get the timesteps.
    ar_pr_timesteps = []
    padded_labels = []
    for i, time in enumerate(curr_times):
        pr_ts = precip_ds.tp.cf.sel(
            time=slice(time[0], time[1]),
        ).cf["time"]
        curr_label = labels.isel(sample_id=[i]).values[0]
        # NOTE: Here we re-map the labels to common geogrpahical feature.
        if remap_labels:
            curr_label = label_dict[ardt_name][curr_label]
        padded_labels.extend([curr_label] * pr_ts.shape[0])
        ar_pr_timesteps.append(pr_ts)

    ar_pr_timesteps_ds = xr.concat(ar_pr_timesteps, dim="valid_time")
    padded_labels = np.asarray(padded_labels)
    assert padded_labels.shape[0] == ar_pr_timesteps_ds.shape[0]

    ar_pr_timesteps_ds = xr.DataArray(
        padded_labels, coords={"valid_time": ar_pr_timesteps_ds.valid_time}
    )
    return ar_pr_timesteps_ds
```

First we have to read in data with the original labels

```{python}
ar_ens_ds = []
for ardt_name in tqdm(ARDT_NAMES):
    ar_path = os.path.join(
        BASE_PATH,
        f"ERA5.{ardt_name}",
        f"ERA5.{ardt_name}.scand_ars.collapsed.1980-2019.zarr",
    )
    label_path = os.path.join(
        BASE_PATH,
        f"ERA5.{ardt_name}",
        f"ERA5.{ardt_name}.collapsed.cluster_labels.zarr",
    )
    ar_ds = xr.open_zarr(ar_path)
    label_ds = xr.open_zarr(label_path)
    ar_ds = subsel_ds(ar_ds, LAT_SLICE, LON_SLICE)

    curr_times = np.asarray(
        list(
            map(
                lambda x: x[-23:].split("-"),
                ar_ds.sample_id.values,
            )
        )
    )
    start_times = pd.to_datetime(curr_times[:, 0], format="%Y%m%dT%H")
    labels = label_ds.labels.data

    ar_ds = ar_ds.assign_coords({"start_time": ("sample_id", start_times)})
    ar_ds = ar_ds.assign_coords({"label": ("sample_id", labels)})

    ar_ens_ds.append(ar_ds)

ar_ens_ds = xr.concat(ar_ens_ds, dim="ardt")
```

Here we compute the average of each ARDTs/cluster.
This is used to perform the field correlation between the clusters of the different ARDTs.

```{python}
cluster_eval = (
    ar_ens_ds.groupby(("label", "ardt", "start_time.year")).sum().mean("year").load()
)
```

Compute the correlation between clusters of different ARDTs.

```{python}
label_dict = {}
for ardt in range(1, 4):
    sub_res = {}
    for true_cluster in range(4):
        best_score = 0
        for test_cluster in range(4):
            score = compute_spatial_correltion(
                cluster_eval.isel(ardt=0, label=test_cluster).ar_tracked_id,
                cluster_eval.isel(ardt=ardt, label=true_cluster).ar_tracked_id,
            )
            if score > best_score:
                best_score = score
                sub_res[true_cluster] = test_cluster
        label_dict[ARDT_NAMES[ardt]] = sub_res

label_dict["Mundhenk_v3"] = {
    0: 0,
    1: 1,
    2: 2,
    3: 3,
}
```

```{python}
label_dict
```

Here we load in the clustered AR data again, this time we also re-map the cluster labels.

```{python}
ar_ens_ds = []
for ardt_name in tqdm(ARDT_NAMES):
    ar_path = os.path.join(
        BASE_PATH,
        f"ERA5.{ardt_name}",
        f"ERA5.{ardt_name}.scand_ars.collapsed.1980-2019.zarr",
    )
    label_path = os.path.join(
        BASE_PATH,
        f"ERA5.{ardt_name}",
        f"ERA5.{ardt_name}.collapsed.cluster_labels.zarr",
    )
    ar_ds = xr.open_zarr(ar_path)
    label_ds = xr.open_zarr(label_path)
    ar_ds = subsel_ds(ar_ds, LAT_SLICE, LON_SLICE)

    curr_times = np.asarray(
        list(
            map(
                lambda x: x[-23:].split("-"),
                ar_ds.sample_id.values,
            )
        )
    )
    start_times = pd.to_datetime(curr_times[:, 0], format="%Y%m%dT%H")
    labels = remap_labels(label_ds.labels.data, label_dict[ardt_name])

    ar_ds = ar_ds.assign_coords({"ardt": [ardt_name]})
    ar_ds = ar_ds.assign_coords({"start_time": ("sample_id", start_times)})
    ar_ds = ar_ds.assign_coords({"label": ("sample_id", labels)})

    ar_ens_ds.append(ar_ds)

ar_ens_ds = xr.concat(ar_ens_ds, dim="ardt")
```

Compute the new cluster evaluation and plot the clusters for all ARDTs to see how well they match.

```{python}
cluster_eval = (
    ar_ens_ds.groupby(("label", "ardt", "start_time.year")).sum().mean("year").load()
)
```

```{python}
n_clusters = 4
fig, ax = pplt.subplots(
    nrows=n_clusters,
    ncols=4,
    proj="lcc",
    proj_kw={"central_longitude": 15},
)
for i in range(n_clusters):
    for j in range(4):
        (cluster_eval.isel(label=i, ardt=j).ar_tracked_id / 1464 * 100).plot(
            ax=ax[i, j]
        )

ax.format(
    coast=True,
    lonlim=LON_SLICE,
    latlim=LAT_SLICE,
    title="",
    rowlabels=list(cluster_eval.label.values.astype(int)),
    collabels=list(cluster_eval.ardt.values),
)
```

## Get time steps

```{python}
ar_ts_ens = []
for ardt_name in tqdm(ARDT_NAMES):
    cluster_path = os.path.join(
        BASE_PATH,
        f"ERA5.{ardt_name}",
        f"ERA5.{ardt_name}.collapsed.cluster_labels.zarr",
    )
    cluster_ds = xr.open_zarr(cluster_path)

    ar_ts_ds = get_cluster_timesteps(cluster_ds, precip_ds, ardt_name)
    ar_ts_ds = ar_ts_ds.assign_coords({"ardt": ardt_name})
    ar_ts_ens.append(ar_ts_ds)
ar_ts_ens = xr.concat(ar_ts_ens, dim="valid_time")
```

Why does 3 algos have more than half duplicate values, where on has only 500???
This is potentially a bug in xarray: https://github.com/pydata/xarray/issues/8499

```{python}
n_ar_ts_per_ardt = list(map(len, ar_ts_ens.groupby("ardt").groups.values()))
print(n_ar_ts_per_ardt)
```

```{python}
n_ar_ts_per_ardt = list(
    map(len, ar_ts_ens.drop_duplicates("valid_time").groupby("ardt").groups.values())
)
print(n_ar_ts_per_ardt)
```

How many duplicate time steps are there truly?
If we do this outside the loop we get completely different results.

```{python}
for name in ARDT_NAMES:
    ts1 = ar_ts_ens.where(ar_ts_ens.ardt == name, drop=True).shape[0]
    ts2 = (
        ar_ts_ens.where(ar_ts_ens.ardt == name, drop=True)
        .drop_duplicates("valid_time")
        .shape[0]
    )
    print(ts1, ts2)
```

# AR precipitation

Here we just get all precipitation time steps from when there are ARs.

We can use these AR time steps to select the precipitation time steps that occur at the same time.
At this points, we don't really need to care about the ensemble members, we can just pile all of it into a single large 1d array? Yes, we group them by ardt later.
- For the histograms yes, but not for the maps? 
- But we can remove the time coordinate for now, but it would be nice to keep the groupby functionality.

```{python}
ar_precip_ens = precip_ds.cf.sel(time=ar_ts_ens.valid_time) * 1e3

n_ar_precip_ens = (
    precip_ds.where(~precip_ds.cf["time"].isin(ar_ts_ens.valid_time), drop=True) * 1e3
)

n_ar_precip_ens.tp.attrs["units"] = "mm"
ar_precip_ens.tp.attrs["units"] = "mm"
```

Remove duplicate time steps.

```{python}
dedup = []
for name in ARDT_NAMES:
    dedup.append(
        ar_precip_ens.where(ar_precip_ens.ardt == name, drop=True).drop_duplicates(
            "valid_time"
        )
    )
ar_precip_ens = xr.concat(dedup, dim="valid_time")
```

Assurance check

```{python}
# These are the timesteps that show in any of the AR algos., but with not repeats.
n_ar_ts_no_dup = ar_precip_ens.cf.drop_duplicates("time").cf["time"].shape[0]
# Timesteps during AR and not during AR.
total_timesteps = n_ar_ts_no_dup + n_ar_precip_ens.cf["time"].shape[0]
# Should be equal the total number time steps between 1980 and 2019.
assert total_timesteps == precip_ds.cf["time"].shape[0]
```

## Tests
What does the total precipitation for the different ARDTs look like?
What we want to check here is if the respective precipitation charts look reasonable with regards to their frequencies.

```{python}
tp_test = ar_precip_ens.cf.groupby(["time.year", "ardt"]).sum().mean("year").load()
```

```{python}
ncols = 4
fig, ax = pplt.subplots(
    ncols=ncols,
    proj=ncols * ["lcc"],
    proj_kw={"central_longitude": 15},
    abc=True,
)
cmap = pplt.Colormap("oslo_r", right=0.8)
for i in range(4):
    (
        # tp_test.isel(ardt=i).tp
        (tp_test.isel(ardt=i).tp / ann_avg_precip_ds * 100).plot(
            ax=ax[i],
            cmap=cmap,
            vmin=0,
            rasterized=True,
            cbar_kwargs={"label": "Total precip. fraction [%]"},
        )
    )

ax.format(
    coast=True,
    reso="med",
    # title=ARDT_NAMES,
    lonlim=LON_SLICE,
    latlim=LAT_SLICE,
)
```

This is actually quite interesting.
Why is there such a large spatial difference between the average precipitation field for the different algorithms?
Also, important to remember that this only indicates that there are ARs somewhere in the region when this precipitation occurs.
It doesn't mean that the ARs and precipitation are spatially co-located.
This is why we compute the spatial correlation later.
Wonder if it does make sense to compute some type of space-time correlation?

```{python}
fig.savefig("../../figures/tp_per_ardt_check.pdf")
```

# Ensemble AR frequency and precipitation

## Annual average

```{python}
n_ar_ann_avg_pr = (
    n_ar_precip_ens.cf.groupby("time.year").sum().mean("year").tp.compute()
)

ar_ann_avg_pr = (
    ar_precip_ens.cf.groupby(["time.year", "ardt"])
    .sum()
    .mean("year")
    .mean("ardt")
    .tp.compute()
)
ar_ann_std_pr = (
    ar_precip_ens.cf.groupby(["time.year", "ardt"])
    .sum()
    .mean("year")
    .std("ardt")
    .tp.compute()
)
```

How many AR time steps do we get during the average year?

```{python}
ar_ann_avg_frq = (
    ar_ens_ds.groupby(["ardt", "start_time.year"])
    .sum()
    .mean("year")
    .mean("ardt")
    .load()
)
```

```{python}
corr = compute_spatial_correltion(
    ar_ann_avg_frq.ar_tracked_id, (ar_ann_avg_pr / ann_avg_precip_ds * 100)
)
```

```{python}
n_year_timesteps = 1464
fig, axs = pplt.subplots(
    figwidth="8.3cm",
    nrows=3,
    # proj=2 * ["nsper"],
    # proj_kw={"lon_0": 14, "lat_0": 65},
    proj=3 * ["lcc"],
    proj_kw={"central_longitude": 15},
    abc=True,
)
(ar_ann_avg_frq.ar_tracked_id / n_year_timesteps * 100).plot(
    ax=axs[0], vmin=0, cbar_kwargs={"label": "AR frequency [%]"}, rasterized=True
)
# vmax = max(n_ar_ann_avg.max().values, ar_ann_avg.max().values)
cm = ar_ann_avg_pr.plot(
    ax=axs[1], vmin=0, cmap="oslor_r", add_colorbar=False, rasterized=True
)
# cm = ar_ann_avg.plot(ax=axs[1], vmin=0, vmax=vmax, cmap="oslor_r", add_colorbar=False)
axs[1].colorbar(cm, label="Total annual average\nprecipitation [mm]")

cmap = pplt.Colormap("oslo_r", right=0.8)
(ar_ann_avg_pr / ann_avg_precip_ds * 100).plot(
    ax=axs[2],
    vmin=0,
    cmap=cmap,
    cbar_kwargs={"label": "Total precipitation\nfraction [%]"},
    rasterized=True,
)

axs[2].annotate(
    f"Corr: {corr.values:.2f}",
    (0.95, 0.95),
    xycoords="axes fraction",
    ha="right",
    va="top",
    bbox={"boxstyle": "square", "facecolor": "white"},
)
axs.format(
    coast=True,
    lonlabels=True,
    latlabels=True,
    reso="med",
    lonlim=LON_SLICE,
    latlim=LAT_SLICE,
    title="",
    rowlabels=["AR frequency", "Total AR precip.", "AR precip. fraction"],
    suptitle="Annual average precipitation\nduring ARs [Ensemble avg.]",
    abcloc="ul",
    abcbbox=True,
    abc_kw={"bbox": {"boxstyle": "square", "facecolor": "white"}},
)
```

```{python}
fig.savefig("../../figures/ann_avg_precip_ar_ens.svg")
```

### Tests
Check what the AR frequencies for the different ARDTs look like.
The ensemble average above should reflect the ARDTs below.

```{python}
test = ar_ens_ds.groupby(["ardt", "start_time.year"]).sum().mean("year").load()
```

```{python}
n_total_timesteps = 58440
fig, ax = pplt.subplots(ncols=4)
for i in range(4):
    (test.isel(ardt=i).ar_tracked_id / n_year_timesteps * 100).plot(
        ax=ax[i], rasterized=True
    )
ax.format(coast=True)
```

```{python}
fig.savefig("../../figures/ann_frq_per_ardt_check.pdf")
```

These frequencies seem more reasonable when compared to literature.

Read in a single ARDT collapsed clusters.
Here we don't do any fancy things, e.g. grouping etc.
Want to compare the annual average to a single year.
Should roughly match.

```{python}
test = xr.open_zarr(
    "/data/projects/atmo_rivers_scandinavia/ERA5.Reid500/ERA5.Reid500.scand_ars.collapsed.1980-2019.zarr/"
)
```

```{python}
test = subsel_ds(test, LAT_SLICE, LON_SLICE)
```

```{python}
curr_times = np.asarray(
    list(
        map(
            lambda x: x[-23:].split("-"),
            test.sample_id.values,
        )
    )
)
start_times = pd.to_datetime(curr_times[:, 0], format="%Y%m%dT%H")
test = test.assign_coords({"time": ("sample_id", start_times)})
test = test.set_xindex("time")
test_avg = test.reset_index("time").groupby("time.year").sum().mean("year")
```

The subplots should have roughly equal AR frequencies.

```{python}
fig, ax = pplt.subplots(ncols=2)
(test_avg.ar_tracked_id / 1464 * 100).plot(ax=ax[0])
(test.sel(time="1980").sum("sample_id").ar_tracked_id / 1464 * 100).plot(ax=ax[1])
ax.format(title=["Groupby average", "Single year"])
```

Testing out some trends.

```{python}
# This is the wrong order of operations. We need to groupby year first!
# We know this since the magnitude of the resulting max values doesn't
# correspond to what we see in the maps above.

# trend_test = (
#     test.reset_index("time").cf.max(["latitude", "longitude"]).ar_tracked_id.load()
# )
```

```{python}
weights = np.cos(np.deg2rad(test.lat))
trend_test = (
    test.reset_index("time")
    .groupby("time.year")
    .sum()
    .cf.max(["latitude", "longitude"])
    # .cf.weighted(weights).mean(["latitude", "longitude"])
    .ar_tracked_id.load()
)
```

```{python}
(trend_test.rolling(year=1, center=True).mean() / 1464 * 100).plot()
```

```{python}
hov_test = (
    test.reset_index("time")
    .groupby(["time.year", "time.month"])
    .sum()
    .cf.max(["latitude", "longitude"])
    .ar_tracked_id.load()
)
```

```{python}
(hov_test / (1464 / 12) * 100).plot()
```

### Ensemble std/spread
Groupby `ardt` as before, but now reduce the resulting dimension using the standard deviation instead.

```{python}
ar_ann_avg_std_frq = (
    ar_ens_ds.groupby(["ardt", "start_time.year"]).sum().mean("year").std("ardt").load()
)
```

```{python}
fig, axs = pplt.subplots(
    figwidth="8.3cm",
    nrows=3,
    # proj=2 * ["nsper"],
    # proj_kw={"lon_0": 14, "lat_0": 65},
    proj=3 * ["lcc"],
    proj_kw={"central_longitude": 15},
    abc=True,
)

(ar_ann_avg_std_frq.ar_tracked_id / n_year_timesteps * 100).plot(
    ax=axs[0], vmin=0, cbar_kwargs={"label": "AR frequency [%]"}, rasterized=True
)
# vmax = max(n_ar_ann_avg.max().values, ar_ann_avg.max().values)
cm = ar_ann_std_pr.plot(
    ax=axs[1], vmin=0, cmap="oslor_r", add_colorbar=False, rasterized=True
)
# cm = ar_ann_avg.plot(ax=axs[1], vmin=0, vmax=vmax, cmap="oslor_r", add_colorbar=False)
axs[1].colorbar(cm, label="Total annual average\nprecipitation [mm]")

cmap = pplt.Colormap("oslo_r", right=0.8)
(ar_ann_std_pr / ann_avg_precip_ds * 100).plot(
    ax=axs[2],
    vmin=0,
    cmap=cmap,
    cbar_kwargs={"label": "Total precipitation\nfraction [%]"},
    rasterized=True,
)

axs.format(
    coast=True,
    lonlabels=True,
    latlabels=True,
    reso="med",
    lonlim=LON_SLICE,
    latlim=LAT_SLICE,
    title="",
    rowlabels=["AR frequency", "Total AR precip.", "AR precip. fraction"],
    suptitle="Annual average precipitation\nduring ARs [Ensemble std]",
    abcloc="ul",
    abcbbox=True,
    abc_kw={"bbox": {"boxstyle": "square", "facecolor": "white"}},
)
```

```{python}
fig.savefig("../../figures/ann_avg_precip_ar_ens_std.svg")
```

## Trends

```{python}
ar_ens_ds
```

```{python}
weights = np.cos(np.deg2rad(ar_ens_ds.lat))

ar_trend = (
    ar_ens_ds.groupby(["start_time.year", "ardt"])
    .sum()
    .weighted(weights)
    .ar_tracked_id
    / 1464 # What about leap years?
    * 100
    .mean(["lat", "lon"])
)
ar_avg_trend = ar_trend.mean("ardt")
ar_avg_trend_std = ar_trend.std("ardt")
```

```{python}
ar_trend_max = (
    ar_ens_ds.groupby(["start_time.year", "ardt"])
    .sum()
    .weighted(weights)
    .quantile(0.99, dim=["lat", "lon"])
    .ar_tracked_id
    / 1464 # Leap years.
    * 100
)
ar_avg_trend_max = ar_trend_max.mean("ardt")
ar_avg_trend_max_std = ar_trend_max.std("ardt")
```

Do one line per algo, two panels.

```{python}
alpha = 0.1
fig, ax = pplt.subplots(figwidth="8.3cm")

time = ar_avg_trend.year

# Spatial averages
ar_avg_trend.plot(ax=ax, label="Ens. avg.")
std_min = ar_avg_trend - ar_avg_trend_std  # .isel(quantile=1)
std_p = ar_avg_trend + ar_avg_trend_std  # .isel(quantile=1)
ax.fill_between(time, std_min, std_p, alpha=alpha, label="Ens. std")

# Spatial extremes
ar_avg_trend_max.plot(ax=ax, label="Ens. avg.")
std_min = ar_avg_trend_max - ar_avg_trend_max_std  # .isel(quantile=1)
std_p = ar_avg_trend_max + ar_avg_trend_max_std  # .isel(quantile=1)
ax.fill_between(time, std_min, std_p, alpha=alpha, label="Ens. std")

ax.legend()
ax.format(
    xlabel="Year",
    ylabel="Ann. avg. frequency [%]",
    suptitle="Annual average AR frequency",
    title="",
)
```

```{python}
fig.savefig("../../figures/ar_ann_avg_trends.svg")
```

## Average timestep

Get histograms.
Select only unique time steps, no doubles.

The average rain rate per hour in ERA5 for this domain is ~0.12 mm/h, 0 values not included.
With a maximum ~13mm/h.

Here we compute the spatial average and 75th percentile before doing the histogram.
- Reduces the variability somewhat.

```{python}
ar_precip_ens = ar_precip_ens.compute()
n_ar_precip_ens = n_ar_precip_ens.compute()

ar_precip_ens_nz = ar_precip_ens#.where(ar_precip_ens.tp > 0, drop=True)
n_ar_precip_ens_nz = n_ar_precip_ens#.where(n_ar_precip_ens.tp > 0, drop=True)
```

```{python}
weights = np.cos(np.deg2rad(ar_precip_ens_nz.latitude))

# Both of these reductions are weighted.
# Mean, also go from 6 hours sums to 1 hour averages.
ar_precip_hist = (
    (ar_precip_ens_nz / 6).weighted(weights).mean(["latitude", "longitude"])
)
n_ar_precip_hist = (
    (n_ar_precip_ens_nz / 6).weighted(weights).mean(["latitude", "longitude"])
)

# Max (0.75 quantile)
# No weights for quantiles.
ar_precip_hist_max = (
    (ar_precip_ens_nz / 6)
    .weighted(weights)
    .quantile(0.75, dim=["latitude", "longitude"])
)
n_ar_precip_hist_max = (
    (n_ar_precip_ens_nz / 6)
    .weighted(weights)
    .quantile(0.75, dim=["latitude", "longitude"])
)
```

```{python}
bin_max = np.round(
    max(
        ar_precip_hist.tp.max().compute(),
        n_ar_precip_hist.tp.max().compute(),
        ar_precip_hist_max.tp.max().compute(),
        n_ar_precip_hist_max.tp.max().compute(),
    ),
    decimals=2,
)
```

Get the bin range for the histgrams.

```{python}
hist_bin_range = (0, bin_max.values)
n_bins = 15
```

Histograms for the spatial average.

```{python}
hist_ar, bins = np.histogram(
    ar_precip_hist.tp.data, range=hist_bin_range, bins=n_bins, density=True
)
# hist_ar = hist_ar.compute()
```

```{python}
hist_n_ar, bins = np.histogram(
    n_ar_precip_hist.tp.data, range=hist_bin_range, bins=n_bins, density=True
)
# hist_n_ar = hist_n_ar.compute()
```

Histograms for the 75th percentile.

```{python}
qtile_hist_ar, bins = np.histogram(
    ar_precip_hist_max.tp.data, range=hist_bin_range, bins=n_bins, density=True
)
# qtile_hist_ar = qtile_hist_ar.compute()
```

```{python}
qtile_hist_n_ar, bins = np.histogram(
    n_ar_precip_hist_max.tp.data, range=hist_bin_range, bins=n_bins, density=True
)
# qtile_hist_n_ar = qtile_hist_n_ar.compute()
```

Compute the field for the average time step with and without ARs.

```{python}
# Average
ar_avg_pr_ts = ar_precip_ens_nz.groupby("ardt").mean().mean("ardt").load() / 6
# ar_avg_pr_ts = ar_precip_mh.where(ar_precip_mh.tp > 0).groupby("ardt").mean().mean("ardt").compute()
n_ar_avg_pr_ts = n_ar_precip_ens_nz.cf.mean("time").compute() / 6
```

```{python}
fig, axs = pplt.subplots(
    figwidth="8.3cm",
    # figheight="12cm",
    nrows=3,
    proj=2 * ["lcc"] + [None],
    proj_kw={"central_longitude": 15},
    sharey=False,
    sharex=False,
    abc=True,
)

qtile = 1#0.975
vmax = max(
    ar_avg_pr_ts.tp.quantile(qtile).values, n_ar_avg_pr_ts.tp.quantile(qtile).values
)

cmap = pplt.Colormap("oslo_r", right=0.8)

pr_cm = (n_ar_avg_pr_ts.tp).plot(
    ax=axs[0], cmap=cmap, vmin=0, vmax=vmax, add_colorbar=False, rasterized=True
)

pr_cm = (ar_avg_pr_ts.tp).plot(
    ax=axs[1], cmap=cmap, vmin=0, vmax=vmax, add_colorbar=False, rasterized=True
)
pr_cbar = fig.colorbar(
    pr_cm,
    loc="right",
    rows=(1, 2),
    length=0.7,
    label="Precipitation [mm/h]",
    #extend="max",
)

axs[2].stairs(hist_ar, bins, label="AR [Avg.]", color="C0")
axs[2].stairs(qtile_hist_ar, bins, label="— [3rd qtile]", color="C0", ls="--")

axs[2].stairs(hist_n_ar, bins, label="No AR [Avg.]", color="C1")

axs[2].stairs(qtile_hist_n_ar, bins, label="— [3rd qtile]", color="C1", ls="--")

axs[2].format(
    yscale="log",
    xlabel="Precipitation [mm/h]",
    ylabel="Density",
    ytickloc="l",
    ylabelloc="l",
    title="Histogram of 1 hour precipitation",
)
axs[2].legend(loc="ll", ncols=1)

# Remove titles on maps.
axs[:2].format(title="")
axs.format(
    coast=True,
    reso="med",
    latlabels=True,
    lonlabels=True,
    lonlim=LON_SLICE,
    latlim=LAT_SLICE,
    # title="",
    rowlabels=["Non-AR", "AR", ""],
    suptitle="Average hourly precipitation\n[Ensemble avg.]",
    abcloc="ul",
    abcbbox=True,
    abc_kw={"bbox": {"boxstyle": "square", "facecolor": "white"}},
)
```

```{python}
fig.savefig("../../figures/avg_precip_timestep_ar_ens.svg")
```

What is the difference between the average AR rain rate and the non-AR rain rate?

```{python}
ar_med.tp.mean() / n_ar_med.tp.mean()
```

### Tests

```{python}
test = (
    ar_precip_ens.groupby("ardt").mean().load()  # where(ar_precip_ens_unique.tp > 0)
    / 6
)
```

```{python}
fig, ax = pplt.subplots(ncols=4)
for i in range(4):
    test.isel(ardt=i).tp.plot(ax=ax[i], vmin=0, discrete=False)
```

```{python}
test.mean("ardt").tp.plot(vmin=0)
```

### Time step std.

```{python}
# mean
ar_std = ar_precip_ens_nz.groupby("ardt").mean().std("ardt").compute() / 6
```

```{python}
fig, axs = pplt.subplots(
    figwidth="8.3cm",
    # figheight="12cm",
    nrows=2,
    proj=2 * ["lcc"],
    proj_kw={"central_longitude": 15},
    sharey=False,
    sharex=False,
    abc=True,
)


cmap = pplt.Colormap("oslo_r", right=0.8)
qtile = 1  # 0.95
# vmax = max(ar_avg_pr_ts.tp.quantile(qtile).values, ar_std.tp.quantile(qtile).values)
pr_cm = (ar_avg_pr_ts.tp).plot(
    ax=axs[0],
    cmap=cmap,
    vmin=0,
    # vmax=vmax,
    add_colorbar=False,
    rasterized=True,
)
pr_cbar = axs[0].colorbar(pr_cm, loc="right", label="Precipitation [mm/h]")

pr_cm = (ar_std.tp).plot(
    ax=axs[1],
    cmap=cmap,
    vmin=0,
    # vmax=vmax,
    add_colorbar=False,
    rasterized=True,
)
pr_cbar = axs[1].colorbar(pr_cm, loc="right", label="Precipitation [mm/h]")

axs.format(
    coast=True,
    reso="med",
    lonlim=LON_SLICE,
    latlim=LAT_SLICE,
    lonlabels=True,
    latlabels=True,
    title="",
    rowlabels=[
        "Avg",
        "Std.",
    ],
    suptitle="Average hourly precipitation [Ensemble avg. & std.]",
    abcloc="ul",
    abcbbox=True,
    abc_kw={"bbox": {"boxstyle": "square", "facecolor": "white"}},
)
```

```{python}
fig.savefig("../../figures/avg_std_precip_timestep_ar_ens.svg")
```

# Cluster ensemble

Could we have a start and end time coordinate for the samples? In order to be able to index by time?

```{python}
dedup = []
for name in ARDT_NAMES:
    dedup.append(
        ar_ts_ens.where(ar_ts_ens.ardt == name, drop=True).drop_duplicates("valid_time")
    )
ar_ts_ens = xr.concat(dedup, dim="valid_time")
```

```{python}
ar_precip_ens = ar_precip_ens.assign_coords({"label": ar_ts_ens})
```

```{python}
# NOTE: Need to divide by 4 since we have 4 ARDTs?
ar_precip_ens_avg = (
    # ar_precip_ens.groupby(["label", "valid_time.year"]).sum().mean("year") / 4
    ar_precip_ens.groupby(["label", "ardt", "valid_time.year"])
    .sum()
    .mean("year")
    .mean("ardt")
).load()
ar_precip_ens_avg_ts = ar_precip_ens.groupby(["label"]).mean().load()
```

```{python}
grouped_ars = (
    ar_ens_ds.groupby(["start_time.year", "label", "ardt"]).sum().mean(["year", "ardt"])
)
```

```{python}
grouped_ars = grouped_ars.load()
```

## Annual average and total precipitation fraction

```{python}
correlations = []
for label in range(4):
    corr = compute_spatial_correltion(
        grouped_ars.isel(label=label).ar_tracked_id,
        ar_precip_ens_avg.isel(label=label).tp / ann_avg_precip_ds,
    )
    correlations.append(corr)
correlations = xr.concat(correlations, dim="label")
```

```{python}
axs[0, 0].colorbar()
```

```{python}
fig, axs = pplt.subplots(
    figwidth="12cm",
    nrows=4,
    ncols=2,
    proj=4 * 2 * ["lcc"],
    proj_kw={"central_longitude": 15},
    sharey=True,
    sharex=True,
    abc=True,
)
vmax = ar_precip_ens_avg.tp.max().values
cmap = pplt.Colormap("oslo_r", right=0.8)
for i in range(4):
    cm = (grouped_ars.isel(label=i).ar_tracked_id / 1464 * 100).plot(
        ax=axs[i, 0],
        vmin=0,
        rasterized=True,
        add_colorbar=False,
    )
    axs[i, 0].colorbar(cm, label="Frequncy [%]", width=0.15, locator="multiple", locator_kw={"base": 0.5})
    curr_tp = ar_precip_ens_avg.isel(label=i).tp
    cm = (curr_tp / ann_avg_precip_ds * 100).plot(
        ax=axs[i, 1],
        vmin=0,
        # vmax=16,
        cmap=cmap,
        rasterized=True,
        add_colorbar=False,
        # cbar_kwargs={"label": "Total precipitation\nfraction [%]"},
    )
    axs[i, 1].annotate(
        f"Corr: {correlations.isel(label=i).values:.2f}",
        (0.95, 0.95),
        xycoords="axes fraction",
        ha="right",
        va="top",
        bbox={"boxstyle": "square", "facecolor": "white"},
    )
    axs[i, 1].colorbar(cm, label="Total precipitation\nfraction [%]", width=0.15)
    axs[i, 0].format(latlabels=True)
    if i == 3:
        axs[i, 0].format(latlabels=True, lonlabels=True)
        axs[i, 1].format(lonlabels=True)


axs.format(
    coast=True,
    reso="med",
    lonlim=LON_SLICE,
    latlim=LAT_SLICE,
    title="",
    abcloc="ul",
    abcbbox=True,
    abc_kw={"bbox": {"boxstyle": "square", "facecolor": "white"}},
    rowlabels=[f"Cluster {i}" for i in range(1, 5)],
    collabels=["AR Frequency", "Total precipitation fraction"],
    suptitle="AR clusters [Ensemble avg.]",
)
```

```{python}
fig.savefig("../../figures/ann_avg_precip_ar_ens_per_cluster.svg")
```

### Tests

```{python}
test = (
    ar_ens_ds.groupby(["start_time.year", "label", "ardt"]).sum().mean(["year"])
).load()
```

Here are individual frequencies for clusters and ardts.
The mean of a column should match the average for a cluster above.

```{python}
fig, ax = pplt.subplots(ncols=4, nrows=4)
for i in range(4):
    for j in range(4):
        (test.ar_tracked_id.isel(ardt=j, label=i) / 14.64).plot(ax=ax[i, j])
```

```{python}
test = test.sum("label")
```

We expect these plots to roughly match what we've seen in the tests for the annual averages.

```{python}
fig, ax = pplt.subplots(ncols=4)
for i in range(4):
    (test.ar_tracked_id.isel(ardt=i) / 14.64).plot(ax=ax[i])
```

Checking total precipitation estimates.

```{python}
tp_test = ar_precip_ens.where(ar_precip_ens.ardt == "Mundhenk_v3", drop=True)
```

Assumed that dates only appear once in a label?
That this assert-statement fails indicates otherwise.

```{python}
assert (
    tp_test.valid_time.shape[0]
    == tp_test.drop_duplicates("valid_time").valid_time.shape[0]
)
```

```{python}
tp_test = (
    tp_test.drop_duplicates("valid_time")
    .cf.groupby(["time.year", "label"])
    .sum()
    .mean("year")
    .load()
)
```

These percentages should be in the ballpark of the ARDT mean above?

```{python}
fig, ax = pplt.subplots(ncols=4)
cmap = pplt.Colormap("oslo_r", right=0.8)
for i in range(4):
    (tp_test.isel(label=i).tp / ann_avg_precip_ds * 100).plot(
        ax=ax[i], vmin=0, cmap=cmap
    )
```

This should be equal the corresponding test for [AR precipitation](#AR-precipitation)

```{python}
(tp_test.sum("label").tp / ann_avg_precip_ds * 100).plot()
```

### Std

```{python}
ar_precip_cluster_std = (
    ar_precip_ens.groupby(["ardt", "label", "valid_time.year"])
    .sum()
    .mean("year")
    .std("ardt")
)
```

```{python}
ar_precip_cluster_std = ar_precip_cluster_std.load()
```

```{python}
freq_cluster_std = (
    ar_ens_ds.groupby(["ardt", "label", "start_time.year"])
    # ar_ens_ds.where(ar_ens_ds.ardt!="Reid500", drop=True).groupby(["ardt", "label", "start_time.year"])
    .sum()
    .mean("year")
    .std("ardt")
    .load()
)
```

```{python}
fig, axs = pplt.subplots(
    figwidth="12cm",
    nrows=4,
    ncols=2,
    proj=4 * 2 * ["lcc"],
    proj_kw={"central_longitude": 15},
    sharey=False,
    sharex=False,
    abc=True,
)
vmax = ar_precip_ens_avg.tp.max().values
cmap = pplt.Colormap("oslo_r", right=0.8)
for i in range(4):
    cm = (freq_cluster_std.isel(label=i).ar_tracked_id / 1464 * 100).plot(
        ax=axs[i, 0],
        vmin=0,
        rasterized=True,
        add_colorbar=False,
    )
    axs[i, 0].colorbar(cm, label="Frequncy [%]", width=0.15)
    #axs[i, 0].colorbar(cm, label="Frequncy [%]", width=0.15, locator="multiple", locator_kw={"base": 0.5})
    curr_tp = ar_precip_cluster_std.isel(label=i).tp
    cm = (curr_tp / ann_avg_precip_ds * 100).plot(
        ax=axs[i, 1],
        vmin=0,
        # vmax=16,
        cmap=cmap,
        rasterized=True,
        add_colorbar=False,
        # cbar_kwargs={"label": "Total precipitation\nfraction [%]"},
    )
    axs[i, 1].colorbar(cm, label="Total precipitation\nfraction [%]", width=0.15)
    axs[i, 0].format(latlabels=True)
    if i == 3:
        axs[i, 0].format(latlabels=True, lonlabels=True)
        axs[i, 1].format(lonlabels=True)


axs.format(
    coast=True,
    reso="med",
    lonlim=LON_SLICE,
    latlim=LAT_SLICE,
    title="",
    abcloc="ul",
    abcbbox=True,
    abc_kw={"bbox": {"boxstyle": "square", "facecolor": "white"}},
    rowlabels=[f"Cluster {i}" for i in range(1, 5)],
    collabels=["AR Frequency", "Total precipitation fraction"],
    suptitle="AR clusters [Ensemble std.]",
)
```

```{python}
fig.savefig("../../figures/ann_std_precip_ar_ens_per_cluster.svg")
```

## Average time step

```{python}
correlations = []
for label in range(4):
    corr = compute_spatial_correltion(
        grouped_ars.isel(label=label).ar_tracked_id / 1464,
        ar_precip_ens_avg_ts.isel(label=label).tp / 6,
    )
    correlations.append(corr)
correlations = xr.concat(correlations, dim="label")
```

```{python}
grouped_ars.isel(label=0)
```

```{python}
group_avg_ar = ar_ens_ds.groupby(["label"]).mean().load()
```

```{python}
fig, axs = pplt.subplots(
    figwidth="12cm",
    nrows=4,
    ncols=2,
    proj=4 * 2 * ["lcc"],
    proj_kw={"central_longitude": 15},
    sharey=False,
    sharex=False,
    abc=True,
)
vmax = ar_precip_ens_avg.tp.max().values
for i in range(4):
    cm = (grouped_ars.isel(label=i).ar_tracked_id / 1464 * 100).plot(
        # cm = (group_avg_ar.isel(label=i).ar_tracked_id).plot(
        ax=axs[i, 0],
        vmin=0,
        rasterized=True,
        add_colorbar=False,
        # cbar_kwargs={"label": "Frequency [%]"},
    )
    axs[i, 0].colorbar(cm, label="Frequency [%]", width=0.15)

    cm = (ar_precip_ens_avg_ts.isel(label=i).tp / 6).plot(
        ax=axs[i, 1], vmin=0, cmap="oslo_r", rasterized=True, add_colorbar=False
    )
    axs[i, 1].colorbar(cm, label="Precipitation [mm/h]", width=0.15)

    axs[i, 1].annotate(
        f"Corr: {correlations.isel(label=i).values:.2f}",
        (0.95, 0.95),
        xycoords="axes fraction",
        ha="right",
        va="top",
        bbox={"boxstyle": "square", "facecolor": "white"},
    )
    axs[i, 0].format(latlabels=True)
    if i == 3:
        axs[i, 0].format(latlabels=True, lonlabels=True)
        axs[i, 1].format(lonlabels=True)


axs.format(
    coast=True,
    reso="med",
    lonlim=LON_SLICE,
    latlim=LAT_SLICE,
    title="",
    abcloc="ul",
    abcbbox=True,
    abc_kw={"bbox": {"boxstyle": "square", "facecolor": "white"}},
    rowlabels=[f"Cluster {i}" for i in range(1, 5)],
    collabels=["AR Frequency", "Timestep avg. precipitation"],
    suptitle="Ensemble average AR clusters",
)
```

```{python}
fig.savefig("../../figures/avg_precip_ar_timestep_ens_per_cluster.svg")
```

## Clusters grouped by season

```{python}
ar_ens_ds = ar_ens_ds.load()
```

```{python}
season_grouped_ars = (
    ar_ens_ds.groupby(["start_time.season", "start_time.year", "label", "ardt"])
    # ar_ens_ds.groupby(["start_time.season", "start_time.year", "label"])
    .sum()
    .mean("year")
    .mean("ardt")
)
```

```{python}
season_grouped_ars = season_grouped_ars.isel(season=slice(0, -1))
```

Get the number of samples for each season, on average.

```{python}
season_samples = {}
for key, val in (
    precip_ds.sel(valid_time="1981").groupby("valid_time.season").groups.items()
):
    season_samples[key] = len(val)
```

```{python}
season_samples
```

Pre-compute the frequencies.

```{python}
freqs = (
    season_grouped_ars
    # / np.asarray(list(season_samples.values())).reshape((1, 1, -1, 1))
    / 1464
    # / 4
    * 100
)  # /  grouped_ars.ar_tracked_id / 1464 * 100
```

```{python}
season_order = ["DJF", "MAM", "JJA", "SON"]
nrows = 4
ncols = 4
fig, axs = pplt.subplots(
    figwidth="12cm",
    nrows=nrows,
    ncols=ncols,
    proj=nrows * ncols * ["lcc"],
    proj_kw={"central_longitude": 15},
    sharey=False,
    sharex=False,
    abc=True,
)
vmax = np.round(freqs.ar_tracked_id.quantile(0.975).values, 1)

for i in range(nrows):
    # vmax = freqs.isel(label=i).ar_tracked_id.max().values
    for j, season in enumerate(season_order):
        cm = (
            freqs.isel(label=i)
            .sel(season=season)
            .ar_tracked_id.plot(
                ax=axs[i, j], rasterized=True, vmin=0, vmax=vmax, add_colorbar=False
            )
        )
fig.colorbar(
    cm,
    span=(2, 3),
    label="Frequency [%]",
    width=0.1,
    extend="max",
    loc="b",
    rasterized=True,
)

axs.format(
    coast=True,
    reso="med",
    lonlim=LON_SLICE,
    latlim=LAT_SLICE,
    title="",
    abcloc="ul",
    abcbbox=True,
    abc_kw={"bbox": {"boxstyle": "square", "facecolor": "white"}},
    rowlabels=[f"Cluster {i}" for i in range(1, 5)],
    collabels=season_order,
    suptitle="Seasonal AR frequencies [Ensemble avg.]",
)
```

```{python}
fig.savefig("../../figures/ens_avg_seasonal_ar_clusters.svg")
```

### Std

```{python}
season_groups_std = (
    ar_ens_ds.groupby(["ardt", "label", "start_time.season", "start_time.year"])
    .sum()
    .mean("year")
    .std("ardt")
)
```

```{python}
freqs = season_groups_std / 1464 * 100
```

```{python}
season_order = ["DJF", "MAM", "JJA", "SON"]
nrows = 4
ncols = 4
fig, axs = pplt.subplots(
    figwidth="12cm",
    nrows=nrows,
    ncols=ncols,
    proj=nrows * ncols * ["lcc"],
    proj_kw={"central_longitude": 15},
    sharey=False,
    sharex=False,
    abc=True,
)
vmax = np.round(freqs.ar_tracked_id.quantile(0.975).values, 1)
for i in range(nrows):
    for j, season in enumerate(season_order):
        cm = (
            freqs.isel(label=i)
            .sel(season=season)
            .ar_tracked_id.plot(
                ax=axs[i, j], rasterized=True, vmin=0, vmax=vmax, add_colorbar=False
            )
        )
fig.colorbar(
    cm,
    label="Frequency [%]",
    width=0.1,
    extend="max",
    loc="b",
    span=(2, 3),
    rasterized=True,
)

axs.format(
    coast=True,
    reso="med",
    lonlim=LON_SLICE,
    latlim=LAT_SLICE,
    title="",
    abcloc="ul",
    abcbbox=True,
    abc_kw={"bbox": {"boxstyle": "square", "facecolor": "white"}},
    rowlabels=[f"Cluster {i}" for i in range(1, 5)],
    collabels=season_order,
    suptitle="Seasonal AR frequencies [Ensemble std.]",
)
```

```{python}
fig.savefig("../../figures/ens_std_seasonal_ar_clusters.svg")
```

## Cluster grouped by NAO

```{python}
def prepare_nao_ds(path: str | None = None) -> pd.Series:
    """Prepare NAO dataframe."""
    if path is None:
        path = Path(__file__).parent / "../etc/norm_daily_nao_index_1950_2024.txt"

    nao_df: pd.DataFrame = pd.read_csv(
        filepath_or_buffer=path,
        sep=r"\s+",
        header=None,
        names=["year", "month", "day", "nao"],
        dtype={"year": str, "month": str, "day": str, "nao": float},
        na_values="-99.0",
    )
    nao_df["time"] = pd.to_datetime(
        nao_df["year"] + "-" + nao_df["month"] + "-" + nao_df["day"]
    )
    nao_df.index = pd.Index(nao_df.time)
    nao_series = nao_df["nao"]
    nao_series = nao_series.interpolate()

    # Upsample to 6 hourly for convenience with era5 data.
    nao_series = nao_series.resample("6h").ffill()

    # first_timestep = ar_ds.time[:1].to_numpy()[0]
    # last_timestep = ar_ds.time[-1:].to_numpy()[0]

    # # What year do we need?
    # nao_series = nao_series.loc[first_timestep:last_timestep]

    return nao_series


def get_nao_bins(nao_series: pd.Series, midpoint_qtile: float = 0.5) -> np.ndarray:
    """Generate NAO bins with roughly equal number of samples on either side of 0."""
    # TODO: Don't hard code midpoint.
    bins = np.quantile(
        nao_series,
        [
            0,
            midpoint_qtile / 2,
            midpoint_qtile,
            midpoint_qtile + (1 - midpoint_qtile) / 2,
            1,
        ],
    )
    return bins
```

```{python}
nao_series = prepare_nao_ds(path="../etc/norm_daily_nao_index_1950_2024.txt")
nao_series = nao_series.loc["1980":"2019"]
nao_da = xr.DataArray(nao_series)
# nao_bins = get_nao_bins(nao_series)
nao_bins = (-3.5, -0.5, 0, 0.5, 3.5)

hist, bins = np.histogram(nao_series, nao_bins)
```

Select the nao values for dates when we have ARs

```{python}
ar_nao_values = nao_da.sel(time=ar_ens_ds.start_time, method="nearest")
ar_nao_values = ar_nao_values.where(~ar_nao_values.label.isnull())
```

and assign them as coordinate to the ar ensemble

```{python}
ar_ens_ds = ar_ens_ds.assign_coords({"nao": ar_nao_values})
```

Groupby both nao and the cluster label.

```{python}
from xarray.groupers import BinGrouper, TimeResampler, UniqueGrouper
```

```{python}
ar_nao_groups = (
    ar_ens_ds.groupby(
        nao=BinGrouper(nao_bins), label=UniqueGrouper(), ardt=UniqueGrouper()
    )
    .sum()
    .mean("ardt")
)
```

```{python}
# Reshape for broadcasting
nao_counts = hist.reshape((1, 1, -1, 1))
```

Calculate the frequencies

```{python}
freqs = ar_nao_groups / nao_counts * 100
```

```{python}
nrows = 4
ncols = 4
fig, axs = pplt.subplots(
    figwidth="12cm",
    nrows=nrows,
    ncols=ncols,
    proj=nrows * ncols * ["lcc"],
    proj_kw={"central_longitude": 15},
    sharey=False,
    sharex=False,
    abc=True,
)

vmax = np.round(freqs.ar_tracked_id.quantile(0.975).values, 1)
# vmax = 4

for i in range(nrows):
    for j in range(4):
        cm = freqs.isel(label=i, nao_bins=j).ar_tracked_id.plot(
            ax=axs[i, j],
            rasterized=True,
            vmin=0,
            vmax=vmax,
            add_colorbar=False,
        )
fig.colorbar(
    cm,
    label="Frequency [%]",
    width=0.1,
    extend="max",
    loc="b",
    span=(2, 3),
    rasterized=True,
)

axs.format(
    coast=True,
    reso="med",
    lonlim=LON_SLICE,
    latlim=LAT_SLICE,
    title="",
    abcloc="ul",
    abcbbox=True,
    abc_kw={"bbox": {"boxstyle": "square", "facecolor": "white"}},
    rowlabels=[f"Cluster {i}" for i in range(1, 5)],
    collabels=[f"{bin}" for bin in freqs.nao_bins.values],
    suptitle="NAO-grouped AR frequencies [Ensemble avg.]",
)
```

```{python}
fig.savefig("../../figures/ens_avg_nao_ar_clusters.svg")
```

### Std

```{python}
ar_nao_std_groups = (
    ar_ens_ds.groupby(
        nao=BinGrouper(nao_bins), label=UniqueGrouper(), ardt=UniqueGrouper()
    )
    .sum()
    .std("ardt")
)
```

```{python}
freqs = ar_nao_std_groups / nao_counts * 100
```

```{python}
nrows = 4
ncols = 4
fig, axs = pplt.subplots(
    figwidth="12cm",
    nrows=nrows,
    ncols=ncols,
    proj=nrows * ncols * ["lcc"],
    proj_kw={"lon_0": 14, "lat_0": 65},
    sharey=False,
    sharex=False,
    abc=True,
)
vmax = np.round(freqs.ar_tracked_id.quantile(0.975).values, 1)
for i in range(nrows):
    for j in range(4):
        cm = freqs.isel(label=i, nao_bins=j).ar_tracked_id.plot(
            ax=axs[i, j], rasterized=True, vmin=0, vmax=vmax, add_colorbar=False
        )

fig.colorbar(
    cm,
    label="Frequency [%]",
    width=0.1,
    extend="max",
    loc="b",
    span=(2, 3),
    rasterized=True,
)

axs.format(
    coast=True,
    reso="med",
    lonlim=LON_SLICE,
    latlim=LAT_SLICE,
    title="",
    abcloc="ul",
    abcbbox=True,
    abc_kw={"bbox": {"boxstyle": "square", "facecolor": "white"}},
    rowlabels=[f"Cluster {i}" for i in range(1, 5)],
    collabels=[f"{bin}" for bin in freqs.nao_bins.values],
    suptitle="NAO-grouped AR frequencies [Ensemble std.]",
)
```

```{python}
fig.savefig("../../figures/ens_std_nao_ar_clusters.svg")
```

